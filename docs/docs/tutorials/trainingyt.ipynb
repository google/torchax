{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2qVWNg4syO_"
   },
   "source": [
    "\n",
    "\n",
    "Training a PyTorch model with JAX\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb039419"
   },
   "source": [
    "\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "This tutorial notebook is adapted from https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "It will keep the most PyTorch code unchanged (especially the model definition),\n",
    "and will replace the standard PyTorch train loop (`loss.backward()` + `optimizer.step()` pattern)\n",
    "with a JAX train loop (`jax.grad` followed by `optax.apply_updates`).\n",
    "\n",
    "The rest of the tutorial, such as data loading, print loss etc. are kept as close to the original as possible.\n",
    "\n",
    "Dataset and DataLoader\n",
    "----------------------\n",
    "\n",
    "The `Dataset` and `DataLoader` classes encapsulate the process of\n",
    "pulling your data from storage and exposing it to your training loop in\n",
    "batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single\n",
    "instances of data.\n",
    "\n",
    "The `DataLoader` pulls instances of data from the `Dataset` (either\n",
    "automatically or with a sampler that you define), collects them in\n",
    "batches, and returns them for consumption by your training loop. The\n",
    "`DataLoader` works with all kinds of datasets, regardless of the type of\n",
    "data they contain.\n",
    "\n",
    "For this tutorial, we'll be using the Fashion-MNIST dataset provided by\n",
    "TorchVision. We use `torchvision.transforms.Normalize()` to zero-center\n",
    "and normalize the distribution of the image tile content, and download\n",
    "both training and validation data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: torch in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.0.6)\n",
      "Requirement already satisfied: jax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.7.2)\n",
      "Requirement already satisfied: optax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.2.6)\n",
      "Requirement already satisfied: tensorboard in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.7.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (1.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (2.3.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (0.1.91)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (6.32.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: toolz>=1.0.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Optional: install dependencies\n",
    "!pip install matplotlib torch torchax jax optax tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GyJEP81WsyO9"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48h6g79csyPB",
    "outputId": "0563dbdc-b9d9-4636-a13c-5aee818f879a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uV8TxRTsyPC"
   },
   "source": [
    "As always, let's visualize the data as a sanity check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "FObQHGljsyPC",
    "outputId": "46d0be39-7817-4dd2-bb0a-da8c995cbcd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pullover  Sneaker  T-shirt/top  Sandal\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwJeNNVpsyPC"
   },
   "source": [
    "The Model\n",
    "=========\n",
    "\n",
    "The model we'll use in this example is a variant of LeNet-5 - it should\n",
    "be familiar if you've watched the previous videos in this series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MXyOntrgsyPC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0540,  0.0372, -0.0884, -0.0064, -0.0794,  0.0316,  0.1010, -0.0222,\n",
       "         -0.0687, -0.0551],\n",
       "        [ 0.0616,  0.0544, -0.1015, -0.0026, -0.0739,  0.0197,  0.1194, -0.0298,\n",
       "         -0.0590, -0.0507],\n",
       "        [ 0.0712,  0.0501, -0.0858, -0.0071, -0.0659,  0.0366,  0.1072, -0.0182,\n",
       "         -0.0559, -0.0602],\n",
       "        [ 0.0546,  0.0431, -0.0927, -0.0122, -0.0676,  0.0256,  0.0923, -0.0314,\n",
       "         -0.0641, -0.0425]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUzquGuBsyPD"
   },
   "source": [
    "Loss Function\n",
    "=============\n",
    "\n",
    "For this example, we'll be using a cross-entropy loss. For demonstration\n",
    "purposes, we'll create batches of dummy output and label values, run\n",
    "them through the loss function, and examine the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j5HHPvrsyPD",
    "outputId": "b5a6be6c-e0b6-4206-cc60-27a5449cf0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4660, 0.0760, 0.2417, 0.7677, 0.6892, 0.2291, 0.5486, 0.5199, 0.8192,\n",
      "         0.4396],\n",
      "        [0.7648, 0.7830, 0.2474, 0.1293, 0.8044, 0.6449, 0.4883, 0.5324, 0.0416,\n",
      "         0.9139],\n",
      "        [0.9712, 0.3357, 0.2326, 0.3509, 0.7713, 0.2730, 0.4183, 0.8110, 0.1735,\n",
      "         0.0313],\n",
      "        [0.6018, 0.9130, 0.3750, 0.5592, 0.2898, 0.2491, 0.9919, 0.3191, 0.3953,\n",
      "         0.4887]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.4828603267669678\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move model to 'jax' device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
      "WARNING:root:Duplicate op registration for aten.__and__\n"
     ]
    }
   ],
   "source": [
    "import torchax\n",
    "torchax.enable_globally()\n",
    "model.to('jax')\n",
    "images = images.to('jax')\n",
    "dummy_labels = dummy_labels.to('jax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiOna1CzsyPD"
   },
   "source": [
    "Optimizer\n",
    "=========\n",
    "\n",
    "For this example, we'll be using simple [optax](https://optax.readthedocs.io/en/latest/getting_started.html)\n",
    "optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tzcmT1YIsyPD"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "start_learning_rate = 1e-3\n",
    "optimizer = optax.adam(start_learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x15df31760>, update=<function chain.<locals>.update_fn at 0x15df31800>)\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ubeUOe6syPD"
   },
   "source": [
    "The Training Loop\n",
    "=================\n",
    "\n",
    "Below, we have a function that performs one training epoch.\n",
    "\n",
    "First, let's articulate what the training step does.\n",
    "\n",
    "At each training step, we first evaluate the model. the Model is a\n",
    "function that maps the `(weights, input data)` to `prediction`.\n",
    "\n",
    "$$ model: (weights, input) \\mapsto pred $$\n",
    "\n",
    "In PyTorch, we can use [torch.func.functional_call](https://docs.pytorch.org/docs/stable/generated/torch.func.functional_call.html) to call a model \n",
    "with weights passed in as a paramter.\n",
    "\n",
    "The loss is a function that takes the prediction, the label to a real number\n",
    "representing the loss:\n",
    "\n",
    "$$ loss: (pred, label) \\mapsto loss $$\n",
    "\n",
    "To train the model, we a glorified Gradient Descent (in this case Adam), so\n",
    "we need to have another function that represent the gradient of the \n",
    "loss with respect of weights.\n",
    "\n",
    "$$ \\frac {d loss} {d weights}$$\n",
    "\n",
    "Finally, the `train_step` itself is a function that takes (weights, optimizer_state, input_data) to\n",
    "(updated weights, and updated optimizer_states).\n",
    "\n",
    "We can spell out the individual components of a train loop, and use Python to assemble them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()\n",
    "\n",
    "def run_model_and_loss(weights, inputs, labels):\n",
    "    # First call the model with passed in weights\n",
    "    output = torch.func.functional_call(model, weights, args=(inputs, ))\n",
    "    loss = loss_fn(output, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(<class 'jaxlib._jax.ArrayImpl'> 2.292862)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_and_loss(model.state_dict(), images, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the gradient function of it. In JAX, one would use `jax.jit`. \n",
    "However, `jax.jit` need to take a JAX function (function that takes jax.Array as inputs and outputs) as \n",
    "argument, and here `run_model_and_loss` takes torch.Tensor as inputs / outputs.\n",
    "\n",
    "One way to solve this issue is to use `jax_view` from the [torchax.interop module](https://github.com/google/torchax/blob/main/torchax/interop.py)\n",
    "\n",
    "`jax_view` converts a torch function to a jax function.\n",
    "\n",
    "`torchax` has common JAX functions wrapped in the [so they work with torch-functions as well.\n",
    "in this case, we will use `jax_value_and_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchax.interop import jax_view\n",
    "import jax\n",
    "\n",
    "grad_fn_jax = jax.grad( jax_view(run_model_and_loss))\n",
    "\n",
    "grad_fn_jax(jax_view(weights), jax_view(images), jax_view(dummy_labels)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above `grad_fn_jax` is the gradient of `jax_view(run_model_and_loss)` and is a jax function.\n",
    "\n",
    "if instead we wish to make a it into a torch function, we can use `torch_view` on it and it will\n",
    "become a function that takes torch tensors and returns torch tensors.\n",
    "\n",
    "In fact, the pattern of calling, `jax_view` + `jax.value_and_grad` + `torch_view` is common enough that\n",
    "we provided this very wraper as `torchax.interop.jax_value_and_grad` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = torchax.interop.jax_value_and_grad(run_model_and_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assemble the train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ei-0yCbisyPD"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "from torchax.interop import call_jax\n",
    "\n",
    "# Initialize optimizer, we need to call optimizer.init, but\n",
    "# it is a JAX-function (function that takes jax arrays as input),\n",
    "# so we use call_jax to pass it torch values:\n",
    "\n",
    "opt_state = call_jax(optimizer.init, weights)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    global weights\n",
    "    global opt_state\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('jax')\n",
    "        labels = labels.to('jax')\n",
    "\n",
    "        # compute gradients\n",
    "        loss, gradients = grad_fn(weights, inputs, labels)\n",
    "        # compute updates\n",
    "        updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
    "        #apply updates\n",
    "        weights = call_jax(optax.apply_updates, weights, updates)\n",
    "        \n",
    "        # Gather data and report\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        if i > 2000: \n",
    "            break\n",
    "            # NOTE: make it run faster for CI\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will work, however, the grad / optimizer update / apply update is pretty \n",
    "standard; so we have a helper to do exactly that [make_train_step](https://github.com/google/torchax/blob/f41e3de8526f9d4e8410bfb84660faaaf0b3ba4a/torchax/train.py#L28)\n",
    "\n",
    "Now let's use that instead.\n",
    "\n",
    "Having a variable for the function of one training step also allows us to compile it with `jax.jit`.\n",
    "Here we use `interop.jax_jit` which just wraps `jax.jit` with `torch_view` and pass kwargs verbatim to the \n",
    "underlying `jax.jit` as below.\n",
    "\n",
    "We can optionally donate the weight and optmizer state, so XLA can issue in-place updates for those 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from torchax.train import make_train_step\n",
    "\n",
    "\n",
    "# the calling convention to make_train_step is the model_fn\n",
    "# takes weights (trainable params) and buffers (non-trainable params)\n",
    "# separately. because jax.jit will compute gradients wrt the first arg.\n",
    "def model_fn(weights, buffers, data):\n",
    "    return torch.func.functional_call(model, (weights, buffers), data)\n",
    "\n",
    "\n",
    "one_step = make_train_step(\n",
    "    model_fn=model_fn,\n",
    "    loss_fn=loss_fn,\n",
    "    optax_optimizer=optimizer)\n",
    "\n",
    "\n",
    "def one_step(weights, opt_state, inputs, labels):\n",
    "            # compute gradients\n",
    "    loss, gradients = grad_fn(weights, inputs, labels)\n",
    "        # compute updates\n",
    "    updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
    "        #apply updates\n",
    "    weights = call_jax(optax.apply_updates, weights, updates)\n",
    "    return loss, weights, opt_state\n",
    "\n",
    "one_step = torchax.interop.jax_jit(one_step)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    global weights\n",
    "    global opt_state\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('jax')\n",
    "        labels = labels.to('jax')\n",
    "\n",
    "        loss, weights, opt_state = one_step(weights, opt_state, inputs, labels) \n",
    "        # Gather data and report\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        if i > 2000: \n",
    "            break\n",
    "            # NOTE: make it run faster for CI\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTrYAn1LsyPE"
   },
   "source": [
    "Per-Epoch Activity\n",
    "==================\n",
    "\n",
    "There are a couple of things we'll want to do once per epoch:\n",
    "\n",
    "-   Perform validation by checking our relative loss on a set of data\n",
    "    that was not used for training, and report this\n",
    "-   Save a copy of the model\n",
    "\n",
    "Here, we'll do our reporting in TensorBoard. This will require going to\n",
    "the command line to start TensorBoard, and opening it in another browser\n",
    "tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfcNC9UwsyPE",
    "outputId": "ea17f7c0-a586-4519-acb5-d49e2923c14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Linearization failed to produce known values for all output primals. This is typically caused by attempting to differentiate a function uses an operation that does not support reverse-mode autodiff.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[32m     14\u001b[39m model.train(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m avg_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m running_vloss = \u001b[32m0.0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch_index, tb_writer)\u001b[39m\n\u001b[32m     42\u001b[39m inputs = inputs.to(\u001b[33m'\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     43\u001b[39m labels = labels.to(\u001b[33m'\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m loss, weights, opt_state = \u001b[43mone_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[32m     47\u001b[39m running_loss += loss.cpu().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/interop.py:234\u001b[39m, in \u001b[36mcall_jax\u001b[39m\u001b[34m(jax_func, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_jax\u001b[39m(jax_func: JaxCallable, *args: TorchValue,\n\u001b[32m    232\u001b[39m              **kwargs: TorchValue) -> TorchValue:\n\u001b[32m    233\u001b[39m   args, kwargs = jax_view((args, kwargs))\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   res: JaxValue = \u001b[43mjax_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m torch_view(res)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/interop.py:242\u001b[39m, in \u001b[36mcall_torch\u001b[39m\u001b[34m(torch_func, *args, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m args, kwargs = torch_view((args, kwargs))\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torchax.default_env():\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m   res: TorchValue = \u001b[43mtorch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jax_view(res)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mone_step\u001b[39m\u001b[34m(weights, opt_state, inputs, labels)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step\u001b[39m(weights, opt_state, inputs, labels):\n\u001b[32m     19\u001b[39m             \u001b[38;5;66;03m# compute gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     loss, gradients = \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# compute updates\u001b[39;00m\n\u001b[32m     22\u001b[39m     updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/interop.py:234\u001b[39m, in \u001b[36mcall_jax\u001b[39m\u001b[34m(jax_func, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_jax\u001b[39m(jax_func: JaxCallable, *args: TorchValue,\n\u001b[32m    232\u001b[39m              **kwargs: TorchValue) -> TorchValue:\n\u001b[32m    233\u001b[39m   args, kwargs = jax_view((args, kwargs))\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   res: JaxValue = \u001b[43mjax_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m torch_view(res)\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/interop.py:242\u001b[39m, in \u001b[36mcall_torch\u001b[39m\u001b[34m(torch_func, *args, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m args, kwargs = torch_view((args, kwargs))\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torchax.default_env():\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m   res: TorchValue = \u001b[43mtorch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jax_view(res)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrun_model_and_loss\u001b[39m\u001b[34m(weights, inputs, labels)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_model_and_loss\u001b[39m(weights, inputs, labels):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# First call the model with passed in weights\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     loss = loss_fn(output, labels)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/_functorch/functional_call.py:148\u001b[39m, in \u001b[36mfunctional_call\u001b[39m\u001b[34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    144\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstateless\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/utils/stateless.py:282\u001b[39m, in \u001b[36m_functional_call\u001b[39m\u001b[34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[39m\n\u001b[32m    278\u001b[39m     args = (args,)\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[32m    280\u001b[39m     module, parameters_and_buffers, tie_weights=tie_weights, strict=strict\n\u001b[32m    281\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mGarmentClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     18\u001b[39m     x = x.view(-\u001b[32m1\u001b[39m, \u001b[32m16\u001b[39m * \u001b[32m4\u001b[39m * \u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/modules/pooling.py:224\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/_jit_internal.py:627\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/nn/functional.py:814\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_max_pool2d\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m    806\u001b[39m     kernel_size: BroadcastingList2[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     return_indices: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    812\u001b[39m ) -> Tensor:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m            \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    826\u001b[39m         stride = torch.jit.annotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/torch/overrides.py:1725\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1722\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1723\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1724\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1726\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1727\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/tensor.py:249\u001b[39m, in \u001b[36mXLAFunctionMode.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m log_nested(\u001b[38;5;28mself\u001b[39m.env, message):\n\u001b[32m    248\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m OperatorNotFound:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/tensor.py:598\u001b[39m, in \u001b[36mEnvironment.dispatch\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m   kwargs[\u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m op.is_jax_function:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m   res = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    600\u001b[39m   \u001b[38;5;66;03m# enable dispatch mode because this op could be a composite autograd op\u001b[39;00m\n\u001b[32m    601\u001b[39m   \u001b[38;5;66;03m# meaning, it will decompose in C++\u001b[39;00m\n\u001b[32m    602\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/ops/jtorch.py:622\u001b[39m, in \u001b[36m_functional_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stride, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    621\u001b[39m     stride = (stride, stride)\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjaten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/qihqi/torchax/torchax/ops/jaten.py:1367\u001b[39m, in \u001b[36mmax_pool\u001b[39m\u001b[34m(inputs, kernel_size, strides, padding, dilation, ceil_mode, with_index)\u001b[39m\n\u001b[32m   1364\u001b[39m init_val = jnp.array(init_val).astype(inputs.dtype)\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m with_index:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     y = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwindow_dilation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_single_input:\n\u001b[32m   1377\u001b[39m         y = jnp.squeeze(y, axis=\u001b[32m0\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/jax/_src/interpreters/ad.py:1104\u001b[39m, in \u001b[36mlinearize_from_jvp\u001b[39m\u001b[34m(jvp, multiple_results, nonzeros, user_facing_symbolic_zeros, instantiate_input_zeros, primals, params)\u001b[39m\n\u001b[32m   1102\u001b[39m out_primals = [trace.to_jaxpr_tracer(p).pval.get_known() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out_primals]\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out_primals):\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1105\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mLinearization failed to produce known values for all output primals. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1106\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mThis is typically caused by attempting to differentiate a function \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1107\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33muses an operation that does not support reverse-mode autodiff.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1109\u001b[39m out_nzs = [\u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m zero_type \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trace.to_jaxpr_tracer(t).is_known()\n\u001b[32m   1110\u001b[39m            \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m out_tangents]\n\u001b[32m   1111\u001b[39m out_tangent_avals = [get_aval(p).to_tangent_aval() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out_primals]\n",
      "\u001b[31mValueError\u001b[39m: Linearization failed to produce known values for all output primals. This is typically caused by attempting to differentiate a function uses an operation that does not support reverse-mode autodiff."
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to('jax')\n",
    "            vlabels = vlabels.to('jax')\n",
    "            model.load_state_dict(weights) # put the trained weight back to test it\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            if i > 1000:\n",
    "                break\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model checkpoint\n",
    "\n",
    "Currently `torch.save` (which is based on Pickle) are not able to save tensors on 'jax' device. \n",
    "Because JAX arrays cannot be pickled.\n",
    "\n",
    "So now we have 2 strategies for saving:\n",
    "1. convert the tensors on jax devices to plain JAX arrays; then use flax.checkpoint to save the data. You will get an JAX-style checkpoint (directory) if you do so.\n",
    "2. convert the tensors from jax devices to CPU torch.Tensor, then use `torch.save`; you will get a regular pickle based checkpoint if you do so.\n",
    "\n",
    "We recommend 1. and we have provided wrapper in `torchax.save_checkpoint` that does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import orbax.checkpoint as ocp\n",
    "ckpt_dir = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\n",
    "model_path = ckpt_dir / 'state'\n",
    "torchax.save_checkpoint(weights, model_path, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/my-checkpoints/\n",
      "/tmp/my-checkpoints/torch_checkpoint.pkl\n",
      "/tmp/my-checkpoints/state\n",
      "/tmp/my-checkpoints/state/checkpoint_1\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_sharding\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_CHECKPOINT_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas/process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d/64caa9fda45b7e010ab72cc60511e3b4\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/a37364fedb7049546fd77e7acc4bb7a9\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/e52cc72492daf25e6f82c2410145c041\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/40aba937d11193a6432d60078b74d341\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/fba2ca4921e7c7379d08a30a1978ab38\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/16d4ae29aca4975149d33bddfaf7d3cb\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/c5ffe437b0783f5e80283f4edc98ee5d\n"
     ]
    }
   ],
   "source": [
    "!find /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also produce a torch pickle based checkpoint by moving the state_dict to CPU\n",
    "\n",
    "You can do so with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpu_state_dict = jax.tree.map(lambda a: a.jax(), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cpu_state_dict, ckpt_dir / 'torch_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mstate\u001b[m\u001b[m                torch_checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
