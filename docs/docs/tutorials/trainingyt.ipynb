{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2qVWNg4syO_"
   },
   "source": [
    "\n",
    "\n",
    "Training a PyTorch model with JAX\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb039419"
   },
   "source": [
    "\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "This tutorial notebook is adapted from https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "It will keep the most PyTorch code unchanged (especially the model definition),\n",
    "and will replace the standard PyTorch train loop (`loss.backward()` + `optimizer.step()` pattern)\n",
    "with a JAX train loop (`jax.grad` followed by `optax.apply_updates`).\n",
    "\n",
    "The rest of the tutorial, such as data loading, print loss etc. are kept as close to the original as possible.\n",
    "\n",
    "Dataset and DataLoader\n",
    "----------------------\n",
    "\n",
    "The `Dataset` and `DataLoader` classes encapsulate the process of\n",
    "pulling your data from storage and exposing it to your training loop in\n",
    "batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single\n",
    "instances of data.\n",
    "\n",
    "The `DataLoader` pulls instances of data from the `Dataset` (either\n",
    "automatically or with a sampler that you define), collects them in\n",
    "batches, and returns them for consumption by your training loop. The\n",
    "`DataLoader` works with all kinds of datasets, regardless of the type of\n",
    "data they contain.\n",
    "\n",
    "For this tutorial, we'll be using the Fashion-MNIST dataset provided by\n",
    "TorchVision. We use `torchvision.transforms.Normalize()` to zero-center\n",
    "and normalize the distribution of the image tile content, and download\n",
    "both training and validation data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: torch in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.0.6)\n",
      "Requirement already satisfied: jax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.7.2)\n",
      "Requirement already satisfied: optax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.2.6)\n",
      "Requirement already satisfied: tensorboard in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.7.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (1.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (2.3.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (0.1.91)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (6.32.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: toolz>=1.0.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Optional: install dependencies\n",
    "!pip install matplotlib torch torchax jax optax tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GyJEP81WsyO9"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48h6g79csyPB",
    "outputId": "0563dbdc-b9d9-4636-a13c-5aee818f879a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uV8TxRTsyPC"
   },
   "source": [
    "As always, let's visualize the data as a sanity check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "FObQHGljsyPC",
    "outputId": "46d0be39-7817-4dd2-bb0a-da8c995cbcd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5943: UserWarning: Explicitly requested dtype int64 requested in arange is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return _arange(start, stop=stop, step=step, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouser  Pullover  Dress  Sandal\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwJeNNVpsyPC"
   },
   "source": [
    "The Model\n",
    "=========\n",
    "\n",
    "The model we'll use in this example is a variant of LeNet-5 - it should\n",
    "be familiar if you've watched the previous videos in this series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MXyOntrgsyPC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0671, -0.0466, -0.0187,  0.0472, -0.1055, -0.1039, -0.0175, -0.0597,\n",
       "          0.0859, -0.0092],\n",
       "        [ 0.0603, -0.0519, -0.0106,  0.0512, -0.0959, -0.1090, -0.0222, -0.0404,\n",
       "          0.0871, -0.0130],\n",
       "        [ 0.0552, -0.0521, -0.0264,  0.0484, -0.1110, -0.0977, -0.0239, -0.0368,\n",
       "          0.0876, -0.0161],\n",
       "        [ 0.1057, -0.0406, -0.0353,  0.0709, -0.1102, -0.1155,  0.0006, -0.0535,\n",
       "          0.1068, -0.0352]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUzquGuBsyPD"
   },
   "source": [
    "Loss Function\n",
    "=============\n",
    "\n",
    "For this example, we'll be using a cross-entropy loss. For demonstration\n",
    "purposes, we'll create batches of dummy output and label values, run\n",
    "them through the loss function, and examine the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j5HHPvrsyPD",
    "outputId": "b5a6be6c-e0b6-4206-cc60-27a5449cf0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9722, 0.7499, 0.2067, 0.3123, 0.9140, 0.1229, 0.4278, 0.6907, 0.1878,\n",
      "         0.8628],\n",
      "        [0.6991, 0.8526, 0.1795, 0.6230, 0.0241, 0.8825, 0.6644, 0.4816, 0.4227,\n",
      "         0.8474],\n",
      "        [0.5714, 0.0439, 0.7818, 0.1378, 0.3126, 0.8229, 0.8647, 0.2827, 0.8375,\n",
      "         0.7954],\n",
      "        [0.2767, 0.4335, 0.3760, 0.5561, 0.9974, 0.8283, 0.0061, 0.2114, 0.1310,\n",
      "         0.5997]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.374995231628418\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move model to 'jax' device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchax\n",
    "torchax.enable_globally()\n",
    "model.to('jax')\n",
    "images = images.to('jax')\n",
    "dummy_labels = dummy_labels.to('jax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiOna1CzsyPD"
   },
   "source": [
    "Optimizer\n",
    "=========\n",
    "\n",
    "For this example, we'll be using simple [optax](https://optax.readthedocs.io/en/latest/getting_started.html)\n",
    "optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tzcmT1YIsyPD"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "start_learning_rate = 1e-3\n",
    "optimizer = optax.adam(start_learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x168480f40>, update=<function chain.<locals>.update_fn at 0x168480fe0>)\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ubeUOe6syPD"
   },
   "source": [
    "The Training Loop\n",
    "=================\n",
    "\n",
    "Below, we have a function that performs one training epoch.\n",
    "\n",
    "First, let's articulate what the training step does.\n",
    "\n",
    "At each training step, we first evaluate the model. the Model is a\n",
    "function that maps the `(weights, input data)` to `prediction`.\n",
    "\n",
    "$$ model: (weights, input) \\mapsto pred $$\n",
    "\n",
    "In PyTorch, we can use [torch.func.functional_call](https://docs.pytorch.org/docs/stable/generated/torch.func.functional_call.html) to call a model \n",
    "with weights passed in as a paramter.\n",
    "\n",
    "The loss is a function that takes the prediction, the label to a real number\n",
    "representing the loss:\n",
    "\n",
    "$$ loss: (pred, label) \\mapsto loss $$\n",
    "\n",
    "To train the model, we a glorified Gradient Descent (in this case Adam), so\n",
    "we need to have another function that represent the gradient of the \n",
    "loss with respect of weights.\n",
    "\n",
    "$$ \\frac {d loss} {d weights}$$\n",
    "\n",
    "Finally, the `train_step` itself is a function that takes (weights, optimizer_state, input_data) to\n",
    "(updated weights, and updated optimizer_states).\n",
    "\n",
    "We can spell out the individual components of a train loop, and use Python to assemble them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()\n",
    "\n",
    "def run_model_and_loss(weights, inputs, labels):\n",
    "    # First call the model with passed in weights\n",
    "    output = torch.func.functional_call(model, weights, args=(inputs, ))\n",
    "    loss = loss_fn(output, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(<class 'jaxlib._jax.ArrayImpl'> 2.33033)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_and_loss(model.state_dict(), images, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the gradient function of it. In JAX, one would use `jax.jit`. \n",
    "However, `jax.jit` need to take a JAX function (function that takes jax.Array as inputs and outputs) as \n",
    "argument, and here `run_model_and_loss` takes torch.Tensor as inputs / outputs.\n",
    "\n",
    "One way to solve this issue is to use `jax_view` from the [torchax.interop module](https://github.com/google/torchax/blob/main/torchax/interop.py)\n",
    "\n",
    "`jax_view` converts a torch function to a jax function.\n",
    "\n",
    "`torchax` has common JAX functions wrapped in the [so they work with torch-functions as well.\n",
    "in this case, we will use `jax_value_and_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchax.interop import jax_view\n",
    "import jax\n",
    "\n",
    "grad_fn_jax = jax.grad( jax_view(run_model_and_loss))\n",
    "\n",
    "grad_fn_jax(jax_view(weights), jax_view(images), jax_view(dummy_labels)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above `grad_fn_jax` is the gradient of `jax_view(run_model_and_loss)` and is a jax function.\n",
    "\n",
    "if instead we wish to make a it into a torch function, we can use `torch_view` on it and it will\n",
    "become a function that takes torch tensors and returns torch tensors.\n",
    "\n",
    "In fact, the pattern of calling, `jax_view` + `jax.value_and_grad` + `torch_view` is common enough that\n",
    "we provided this very wraper as `torchax.interop.jax_value_and_grad` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = torchax.interop.jax_value_and_grad(run_model_and_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assemble the train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ei-0yCbisyPD"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "from torchax.interop import call_jax\n",
    "\n",
    "# Initialize optimizer, we need to call optimizer.init, but\n",
    "# it is a JAX-function (function that takes jax arrays as input),\n",
    "# so we use call_jax to pass it torch values:\n",
    "\n",
    "opt_state = call_jax(optimizer.init, weights)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    global weights\n",
    "    global opt_state\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('jax')\n",
    "        labels = labels.to('jax')\n",
    "\n",
    "        # compute gradients\n",
    "        loss, gradients = grad_fn(weights, inputs, labels)\n",
    "        # compute updates\n",
    "        updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
    "        #apply updates\n",
    "        weights = call_jax(optax.apply_updates, weights, updates)\n",
    "        \n",
    "        # Gather data and report\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        if i > 2000: \n",
    "            break\n",
    "            # NOTE: make it run faster for CI\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [[[[-0.01919572  0.08173535  0.11195555 -0.0464663  -0.01724706]\n",
       "                 [ 0.00474899 -0.09588833 -0.08292859  0.01614468 -0.07575631]\n",
       "                 [-0.14652589  0.13480428 -0.19805682  0.03487966 -0.15263537]\n",
       "                 [-0.19650117 -0.03486729 -0.13269444  0.19900253 -0.10203419]\n",
       "                 [-0.11847182  0.1167357  -0.07137737  0.17127113  0.00041001]]]\n",
       "              \n",
       "              \n",
       "               [[[ 0.0435169  -0.01835244  0.00851111  0.08337722  0.19144812]\n",
       "                 [-0.00774817 -0.16186371 -0.06935952  0.00296748  0.06117613]\n",
       "                 [-0.09910274  0.07001872  0.14024901  0.17163089 -0.17241679]\n",
       "                 [ 0.03131669 -0.07166276 -0.1258725  -0.03829036  0.11497901]\n",
       "                 [-0.10303264 -0.16092996 -0.00041683  0.14637287  0.05899556]]]\n",
       "              \n",
       "              \n",
       "               [[[ 0.18458724 -0.14367557 -0.02649014  0.17617439  0.08161464]\n",
       "                 [-0.19239782 -0.00667703 -0.00812833 -0.0646889   0.17949168]\n",
       "                 [ 0.06602452  0.00380657 -0.18977754 -0.09130666  0.00719433]\n",
       "                 [-0.01846514  0.09957304  0.07289324  0.07443617 -0.19789854]\n",
       "                 [-0.08196845  0.00799863  0.00934298 -0.05849845  0.14274152]]]\n",
       "              \n",
       "              \n",
       "               [[[-0.07220769  0.16581424 -0.15188038 -0.08171242  0.07206462]\n",
       "                 [ 0.12747581  0.11063457 -0.13432959  0.13748439 -0.06907818]\n",
       "                 [ 0.17811704  0.06490021  0.10164211  0.17231153  0.06336801]\n",
       "                 [-0.1642262  -0.16008155 -0.15443645  0.01030908  0.05846934]\n",
       "                 [-0.08566506  0.13406134  0.00882998 -0.17155151 -0.04046888]]]\n",
       "              \n",
       "              \n",
       "               [[[-0.03523846  0.08330786  0.06066578  0.09701552 -0.05889471]\n",
       "                 [ 0.08205707  0.14877585 -0.02122119  0.09364748 -0.18886133]\n",
       "                 [-0.15017822  0.16790931  0.14803778  0.01380091  0.1176275 ]\n",
       "                 [-0.09493394 -0.14758883  0.05794099  0.1380475  -0.16307485]\n",
       "                 [ 0.03179183 -0.12281742 -0.05465202  0.14990902 -0.12450574]]]\n",
       "              \n",
       "              \n",
       "               [[[-0.14182416  0.12555568 -0.12531126 -0.18894318  0.03391492]\n",
       "                 [ 0.08209484 -0.16689901  0.03744345 -0.16726403  0.09093683]\n",
       "                 [-0.16624923 -0.1432252   0.04578183 -0.07436476  0.04867101]\n",
       "                 [-0.14170328  0.10347042  0.10079246  0.0743763   0.17727736]\n",
       "                 [-0.04970811 -0.10935595 -0.12565647  0.03139305 -0.1393727 ]]]])),\n",
       "             ('conv1.bias',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [ 0.08820458  0.09139679 -0.00596485 -0.00657737  0.02152777  0.08128238])),\n",
       "             ('conv2.weight',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [[[[ 6.73362613e-02  3.87138501e-02 -4.42972481e-02  3.64186168e-02\n",
       "                  -4.96434048e-02]\n",
       "                 [-3.54743041e-02 -4.87623177e-02  1.72384996e-02 -4.60463315e-02\n",
       "                   4.76922579e-02]\n",
       "                 [-5.17881811e-02  5.59514277e-02  4.38902378e-02 -7.01823235e-02\n",
       "                   5.43927215e-02]\n",
       "                 [ 7.74315596e-02 -3.70158777e-02 -1.41546484e-02  4.50938605e-02\n",
       "                  -3.65107730e-02]\n",
       "                 [ 5.15774731e-03 -1.67917963e-02  5.78212142e-02  2.38991901e-02\n",
       "                  -4.92731296e-02]]\n",
       "              \n",
       "                [[-5.81499971e-02 -1.54426107e-02  6.56848848e-02 -3.67951356e-02\n",
       "                   2.70124674e-02]\n",
       "                 [ 6.41122311e-02 -3.85724157e-02  6.02699220e-02 -3.75762698e-03\n",
       "                  -7.71693289e-02]\n",
       "                 [ 8.14183205e-02  3.95930484e-02  2.39964854e-02  7.67315775e-02\n",
       "                  -3.44647169e-02]\n",
       "                 [-1.38023961e-02 -4.88305204e-02  5.27814552e-02 -4.89586815e-02\n",
       "                   3.08608096e-02]\n",
       "                 [ 6.08278962e-04 -6.71892762e-02 -2.95391697e-02 -2.44140881e-03\n",
       "                   2.65612844e-02]]\n",
       "              \n",
       "                [[ 5.34338057e-02 -7.77476607e-03  4.32579452e-03 -6.50712475e-02\n",
       "                  -2.13966761e-02]\n",
       "                 [ 1.32476417e-02 -3.39078507e-03 -1.92689355e-02 -5.20160869e-02\n",
       "                   6.79686368e-02]\n",
       "                 [ 6.36626706e-02  3.54554802e-02 -3.19466665e-02  1.85731729e-03\n",
       "                   3.92643996e-02]\n",
       "                 [ 5.47725856e-02 -6.11828379e-02 -4.78354283e-02  7.15074763e-02\n",
       "                  -2.43913308e-02]\n",
       "                 [-1.55351656e-02 -7.82363340e-02 -6.69803023e-02 -1.54476520e-02\n",
       "                  -6.11133799e-02]]\n",
       "              \n",
       "                [[-7.91026428e-02 -2.60371026e-02  1.51213398e-02  4.39039916e-02\n",
       "                  -3.91073823e-02]\n",
       "                 [-3.74888629e-02 -9.66785382e-03 -8.14054087e-02  5.41627966e-02\n",
       "                  -5.99530004e-02]\n",
       "                 [ 4.56269681e-02 -4.42591533e-02 -3.84180546e-02 -2.49068905e-03\n",
       "                   3.32460762e-03]\n",
       "                 [ 3.86909582e-02 -8.92023090e-03  6.11732304e-02 -3.98777798e-02\n",
       "                   3.14354897e-02]\n",
       "                 [-3.87429036e-02  1.24188429e-02  7.43572935e-02  5.01440056e-02\n",
       "                   6.81168512e-02]]\n",
       "              \n",
       "                [[-1.69688463e-02 -2.13357937e-02 -2.87076854e-03 -3.57139893e-02\n",
       "                  -1.09936493e-02]\n",
       "                 [ 7.78229982e-02 -1.30581418e-02 -2.52462551e-02  3.07481457e-02\n",
       "                   7.18762875e-02]\n",
       "                 [ 6.37997966e-03 -1.78220461e-03  1.70005765e-02 -2.72837859e-02\n",
       "                   7.37907961e-02]\n",
       "                 [-6.60054460e-02  1.32661154e-02 -3.37326303e-02  4.90240119e-02\n",
       "                  -7.82853737e-02]\n",
       "                 [ 1.01291481e-02  5.29995300e-02  4.10493985e-02  2.70612612e-02\n",
       "                   2.39512455e-02]]\n",
       "              \n",
       "                [[ 6.41939119e-02 -1.48079535e-02 -6.28880039e-02 -7.23704249e-02\n",
       "                   1.72020285e-03]\n",
       "                 [ 1.59410574e-02 -6.45669699e-02  5.92266582e-03 -7.67505914e-02\n",
       "                  -3.89656834e-02]\n",
       "                 [-1.03058582e-02  1.67845152e-02 -4.16487530e-02  6.08960614e-02\n",
       "                   6.37837872e-02]\n",
       "                 [-7.82247707e-02 -7.55841881e-02  5.82411885e-02 -5.06528467e-02\n",
       "                  -8.12322274e-02]\n",
       "                 [-6.75299913e-02 -2.92861797e-02 -4.68846783e-02  4.11099717e-02\n",
       "                  -3.19095924e-02]]]\n",
       "              \n",
       "              \n",
       "               [[[-4.47810302e-03 -1.20174279e-02 -6.73583597e-02  3.54000367e-02\n",
       "                  -2.91771255e-03]\n",
       "                 [-3.83127779e-02 -2.86016203e-02 -2.50129551e-02 -7.80006945e-02\n",
       "                   6.18262514e-02]\n",
       "                 [ 7.52452016e-02 -6.20675161e-02 -6.97440058e-02 -7.45900646e-02\n",
       "                  -5.28070033e-02]\n",
       "                 [ 5.17392494e-02  5.54917865e-02  4.27587107e-02  6.19995072e-02\n",
       "                  -6.98826909e-02]\n",
       "                 [ 1.43927271e-02  5.86770214e-02  2.27989964e-02 -3.83637212e-02\n",
       "                  -6.97477981e-02]]\n",
       "              \n",
       "                [[-2.02545784e-02  7.35340687e-03  6.77736523e-03 -2.01828536e-02\n",
       "                   1.44432916e-03]\n",
       "                 [ 6.18134998e-03 -3.57013270e-02 -7.75391757e-02  6.16320521e-02\n",
       "                  -4.78472747e-02]\n",
       "                 [ 7.37686977e-02 -1.45117864e-02 -6.96430206e-02  2.48756446e-02\n",
       "                  -2.90009994e-02]\n",
       "                 [-5.96850514e-02  4.46007680e-03  1.81909613e-02 -3.11759077e-02\n",
       "                   2.14360077e-02]\n",
       "                 [ 6.64207106e-03  7.38978982e-02 -1.02992691e-02 -5.17349951e-02\n",
       "                   6.42197058e-02]]\n",
       "              \n",
       "                [[ 7.99204335e-02 -5.33168204e-02 -7.12945126e-03 -1.39479106e-02\n",
       "                  -4.50810716e-02]\n",
       "                 [ 6.02416843e-02 -3.64886001e-02 -7.02963173e-02  4.02435148e-03\n",
       "                  -4.67644222e-02]\n",
       "                 [-6.33047670e-02 -3.44522968e-02 -1.23308524e-02  7.79555738e-02\n",
       "                  -4.88643534e-02]\n",
       "                 [-1.54596046e-02  7.97369033e-02 -2.55555436e-02 -2.43976386e-03\n",
       "                  -7.15783313e-02]\n",
       "                 [ 7.33446777e-02 -2.05497630e-02 -1.47561040e-02 -5.40744178e-02\n",
       "                   5.70107028e-02]]\n",
       "              \n",
       "                [[ 9.69941914e-03 -4.54792157e-02 -2.34775785e-02  3.28686163e-02\n",
       "                   2.07145195e-02]\n",
       "                 [-3.64512242e-02  3.82026620e-02  2.97626574e-02  3.75900604e-02\n",
       "                   6.63271248e-02]\n",
       "                 [-3.15221176e-02 -4.75481562e-02  1.85026452e-02 -1.56009439e-02\n",
       "                  -2.60571633e-02]\n",
       "                 [ 6.08009174e-02 -2.32747737e-02 -3.88255231e-02 -2.68417905e-04\n",
       "                  -6.86368868e-02]\n",
       "                 [-5.72876967e-02 -2.28137914e-02 -6.36822060e-02 -6.75781816e-02\n",
       "                  -4.52186428e-02]]\n",
       "              \n",
       "                [[ 3.09172533e-02  3.70260216e-02 -5.90238608e-02  7.22053573e-02\n",
       "                  -2.57421620e-02]\n",
       "                 [ 1.81831159e-02  1.28211332e-02  7.74482414e-02  2.01187003e-02\n",
       "                   7.14721382e-02]\n",
       "                 [ 7.37231523e-02 -4.02484201e-02  6.69135004e-02 -5.43529689e-02\n",
       "                  -1.63910427e-05]\n",
       "                 [ 1.07717859e-02  2.27254108e-02 -3.40783782e-02 -5.32751903e-02\n",
       "                  -6.18940182e-02]\n",
       "                 [ 7.61350393e-02  2.12327274e-03 -5.71572371e-02  7.60992691e-02\n",
       "                   2.14327388e-02]]\n",
       "              \n",
       "                [[ 3.63805331e-02  4.96219359e-02 -7.46104866e-02  2.55209506e-02\n",
       "                   2.33716406e-02]\n",
       "                 [ 4.39942665e-02 -3.67134623e-02 -2.71030944e-02  3.14832106e-02\n",
       "                  -4.30987291e-02]\n",
       "                 [ 1.19178351e-02  4.51325700e-02  1.68129373e-02 -2.32600383e-02\n",
       "                  -4.84682061e-02]\n",
       "                 [ 1.57292485e-02 -6.10684976e-02 -1.85032580e-02 -2.87407776e-04\n",
       "                   3.83341722e-02]\n",
       "                 [ 6.30673096e-02 -1.09884320e-02 -6.52156845e-02  1.46736233e-02\n",
       "                  -9.04868264e-03]]]\n",
       "              \n",
       "              \n",
       "               [[[-1.43217510e-02  5.01164570e-02 -8.98351800e-03  4.76191118e-02\n",
       "                  -3.17498781e-02]\n",
       "                 [ 2.60151643e-02 -5.20433709e-02  7.50263855e-02 -7.68404603e-02\n",
       "                  -1.63236877e-03]\n",
       "                 [-1.08302934e-02  5.98401241e-02  3.44220065e-02  4.56461906e-02\n",
       "                   3.76537479e-02]\n",
       "                 [ 1.90251914e-03 -1.85459591e-02  6.24697469e-02  7.53342435e-02\n",
       "                  -4.40578274e-02]\n",
       "                 [-3.31501737e-02  4.12454009e-02  1.23527236e-02  6.58852747e-03\n",
       "                   4.09017056e-02]]\n",
       "              \n",
       "                [[-3.27580422e-03 -7.01510981e-02 -6.14907146e-02  7.69339874e-02\n",
       "                   6.29267991e-02]\n",
       "                 [-4.15525585e-02 -2.38632355e-02 -5.62842228e-02  7.46006554e-04\n",
       "                  -7.73210451e-02]\n",
       "                 [-8.95633269e-03 -1.52231129e-02  7.99435154e-02 -3.04748793e-03\n",
       "                  -3.45874168e-02]\n",
       "                 [ 4.34176512e-02  1.57695841e-02  7.51032680e-02 -7.43650422e-02\n",
       "                  -4.84367972e-03]\n",
       "                 [ 1.09702796e-02  5.58374189e-02  2.17968356e-02  5.54498248e-02\n",
       "                  -3.11336275e-02]]\n",
       "              \n",
       "                [[ 4.38796654e-02  2.86189150e-02 -7.12882280e-02  1.35022085e-02\n",
       "                   4.78858761e-02]\n",
       "                 [-2.78907120e-02 -7.43581653e-02 -3.58040221e-02  7.91409910e-02\n",
       "                  -7.93038458e-02]\n",
       "                 [ 3.20471153e-02 -5.74316122e-02 -5.44958152e-02  2.92387102e-02\n",
       "                  -4.84887809e-02]\n",
       "                 [-2.16552541e-02 -1.44900810e-02  4.10585897e-03 -6.96080104e-02\n",
       "                   2.37621162e-02]\n",
       "                 [-6.05445765e-02 -5.66884689e-02 -2.25801989e-02 -6.79544806e-02\n",
       "                  -3.09718568e-02]]\n",
       "              \n",
       "                [[ 4.83498871e-02  1.96183641e-02 -2.89723352e-02  7.86655471e-02\n",
       "                   1.88887678e-02]\n",
       "                 [ 8.03338140e-02 -2.49139667e-02  5.94184324e-02  1.30414302e-02\n",
       "                  -2.48874221e-02]\n",
       "                 [-7.10624307e-02 -8.10006931e-02 -1.27211129e-02  3.71558852e-02\n",
       "                   2.79328767e-02]\n",
       "                 [ 5.10151722e-02  6.37063384e-02 -1.79636572e-02  7.92025328e-02\n",
       "                   3.94346565e-02]\n",
       "                 [-2.93459911e-02  5.99901527e-02  3.95842688e-03  3.54431868e-02\n",
       "                   3.28726048e-04]]\n",
       "              \n",
       "                [[ 4.80491929e-02  4.00149710e-02 -2.16310471e-02 -7.88189247e-02\n",
       "                   3.54534239e-02]\n",
       "                 [-1.71393063e-02  1.20181087e-02  4.59722504e-02 -7.26389959e-02\n",
       "                  -2.77842768e-02]\n",
       "                 [-7.07442164e-02  6.62110895e-02 -3.59830298e-02 -4.46546897e-02\n",
       "                  -7.19937906e-02]\n",
       "                 [-7.03859776e-02 -5.65257668e-02 -1.06411641e-02  1.65922120e-02\n",
       "                  -1.17017543e-02]\n",
       "                 [-1.03548365e-02  8.16019997e-02 -5.89396164e-04 -2.11446192e-02\n",
       "                   2.49164477e-02]]\n",
       "              \n",
       "                [[ 1.30567597e-02 -4.04544361e-02  2.33323276e-02  2.46928427e-02\n",
       "                  -2.33104471e-02]\n",
       "                 [-3.03288493e-02  6.99246824e-02  5.94706908e-02  3.19386385e-02\n",
       "                  -3.30318441e-03]\n",
       "                 [ 2.77503654e-02  1.00946333e-02 -7.28809312e-02  3.69449817e-02\n",
       "                   1.80047322e-02]\n",
       "                 [ 4.09046933e-02 -6.99532703e-02 -6.06499128e-02  4.53759618e-02\n",
       "                   4.06803377e-02]\n",
       "                 [-3.11853201e-03 -8.07784572e-02  4.59533110e-02  7.01070502e-02\n",
       "                  -3.43708396e-02]]]\n",
       "              \n",
       "              \n",
       "               ...\n",
       "              \n",
       "              \n",
       "               [[[ 2.50419509e-02  2.98034791e-02  3.25829498e-02 -4.06818762e-02\n",
       "                  -6.18559495e-02]\n",
       "                 [ 5.26606031e-02  6.74183890e-02 -4.13499363e-02 -3.39778215e-02\n",
       "                   2.02266630e-02]\n",
       "                 [ 3.86772640e-02 -6.26043975e-02  4.28928845e-02  1.49924699e-02\n",
       "                   7.02179968e-02]\n",
       "                 [-7.35785533e-03  6.21489435e-02  5.07035479e-02  4.82080914e-02\n",
       "                  -7.13334465e-03]\n",
       "                 [-3.74478474e-02 -7.55244121e-02  5.32650873e-02 -7.35691786e-02\n",
       "                  -3.05685736e-02]]\n",
       "              \n",
       "                [[ 2.16159895e-02  1.86680835e-02 -1.26444725e-02 -2.18901201e-03\n",
       "                   6.73603043e-02]\n",
       "                 [ 2.34088805e-02 -3.78577895e-02  6.29769340e-02 -8.03570077e-02\n",
       "                   8.06205869e-02]\n",
       "                 [ 6.13410994e-02  4.68234159e-02  2.56213117e-02  4.69697192e-02\n",
       "                  -4.02722275e-03]\n",
       "                 [ 5.12910187e-02 -1.65316705e-02  5.69330491e-02  8.08019489e-02\n",
       "                  -6.42339140e-02]\n",
       "                 [ 6.10964894e-02  4.42643799e-02 -2.47084163e-02  1.69614293e-02\n",
       "                  -7.77330771e-02]]\n",
       "              \n",
       "                [[ 1.69375241e-02  3.94939631e-02  2.48651635e-02  2.36880537e-02\n",
       "                  -3.85086425e-02]\n",
       "                 [ 4.19375710e-02  3.18815783e-02  1.78707726e-02  6.14984594e-02\n",
       "                   7.86293373e-02]\n",
       "                 [ 2.73744911e-02 -1.12438751e-02 -3.62883955e-02  1.44589432e-02\n",
       "                   6.15586229e-02]\n",
       "                 [ 9.61740501e-03 -2.03504227e-02  3.29216123e-02 -2.58453358e-02\n",
       "                  -4.73283939e-02]\n",
       "                 [-2.30949111e-02 -7.00651482e-02 -4.86947708e-02  1.34563744e-02\n",
       "                   1.53925512e-02]]\n",
       "              \n",
       "                [[-3.71898548e-03 -2.72780042e-02 -2.95658782e-02 -4.51291613e-02\n",
       "                  -5.39992675e-02]\n",
       "                 [ 5.86275868e-02 -5.82055859e-02  2.65621319e-02  2.87835859e-02\n",
       "                   3.65149193e-02]\n",
       "                 [ 5.23946770e-02 -4.20573307e-03 -5.00293933e-02  3.37650515e-02\n",
       "                  -3.65724079e-02]\n",
       "                 [ 2.81785764e-02  4.98014465e-02  7.71355480e-02  2.04472616e-02\n",
       "                  -3.70235406e-02]\n",
       "                 [ 5.83065115e-02  2.93662753e-02  7.09237978e-02 -5.82772419e-02\n",
       "                   6.99171573e-02]]\n",
       "              \n",
       "                [[-3.24073583e-02  6.73629018e-03  2.77654908e-04  5.52304164e-02\n",
       "                  -9.44721699e-03]\n",
       "                 [-2.89682671e-02  1.95462015e-02  1.17698004e-02  6.36751875e-02\n",
       "                   5.45355491e-03]\n",
       "                 [ 6.95913061e-02  3.75725701e-02 -3.21539901e-02 -3.05261258e-02\n",
       "                  -4.28423285e-02]\n",
       "                 [-8.06299895e-02 -1.63906924e-02 -5.89029416e-02  1.06993597e-02\n",
       "                   6.90690428e-02]\n",
       "                 [-2.28187926e-02  3.68964598e-02  2.55183801e-02  5.63090406e-02\n",
       "                  -1.89622166e-03]]\n",
       "              \n",
       "                [[-5.63786253e-02 -3.40037234e-02 -1.65527053e-02  1.73788853e-02\n",
       "                   7.91705176e-02]\n",
       "                 [-6.04647920e-02 -1.01259658e-02 -7.83316791e-02 -6.49399757e-02\n",
       "                  -3.88651565e-02]\n",
       "                 [-4.84575182e-02  2.43097171e-02  5.91686927e-02 -1.12979151e-02\n",
       "                  -7.19376579e-02]\n",
       "                 [-6.93440884e-02  5.65671623e-02  6.75567612e-02 -4.30529118e-02\n",
       "                  -8.11866671e-03]\n",
       "                 [-4.35004458e-02  6.17990382e-02  6.92659756e-03 -3.51001993e-02\n",
       "                  -7.29366541e-02]]]\n",
       "              \n",
       "              \n",
       "               [[[ 1.36700124e-02  7.01628998e-02 -3.09917238e-03  4.47221622e-02\n",
       "                  -4.16666418e-02]\n",
       "                 [ 6.42014965e-02 -2.17014877e-03  2.59630810e-02 -2.45727319e-02\n",
       "                  -5.15904836e-02]\n",
       "                 [-7.71625862e-02  3.01146768e-02 -3.29343528e-02  5.99321350e-03\n",
       "                   1.13994246e-02]\n",
       "                 [-9.46955476e-03  5.51963598e-02 -2.75633782e-02  8.09245929e-02\n",
       "                  -7.31981397e-02]\n",
       "                 [ 3.35143954e-02 -6.11632541e-02  5.98231293e-02 -6.98512569e-02\n",
       "                  -3.49276364e-02]]\n",
       "              \n",
       "                [[-2.34848401e-03  4.36662436e-02 -2.10828613e-02  7.89600685e-02\n",
       "                  -5.10650352e-02]\n",
       "                 [-3.50376056e-03  2.22239364e-02  2.93581281e-02 -5.96456714e-02\n",
       "                   6.16154186e-02]\n",
       "                 [-4.67111133e-02 -8.48343538e-04  7.88454860e-02 -6.69378564e-02\n",
       "                   4.35692295e-02]\n",
       "                 [-5.29692024e-02  2.18728632e-02  4.61844988e-02 -8.15890431e-02\n",
       "                   2.42090542e-02]\n",
       "                 [-3.89660150e-02 -5.22242859e-02  5.70687726e-02  1.88327730e-02\n",
       "                   3.70226912e-02]]\n",
       "              \n",
       "                [[-2.29072999e-02 -7.84052685e-02  3.73966694e-02 -1.97899844e-02\n",
       "                   4.77293804e-02]\n",
       "                 [ 1.74093805e-02 -1.30503261e-02  8.15234426e-03 -1.60690621e-02\n",
       "                  -2.65747067e-02]\n",
       "                 [ 6.95212409e-02 -7.16685578e-02  4.21120636e-02 -4.46488969e-02\n",
       "                  -3.46953794e-03]\n",
       "                 [ 2.56886669e-02  4.22990136e-03 -7.95597732e-02 -7.50618242e-03\n",
       "                   7.09575713e-02]\n",
       "                 [-2.50896830e-02 -7.96517078e-03  2.00032238e-02  5.16402125e-02\n",
       "                  -5.64796105e-02]]\n",
       "              \n",
       "                [[ 4.63752039e-02  3.46936956e-02  5.86382635e-02 -5.40579408e-02\n",
       "                   5.39568886e-02]\n",
       "                 [-2.17861198e-02 -4.65462776e-03 -6.12228885e-02  5.53563759e-02\n",
       "                  -7.74612799e-02]\n",
       "                 [-4.11000699e-02 -6.77479804e-02 -2.84450669e-02 -5.01099862e-02\n",
       "                   8.10918510e-02]\n",
       "                 [-7.72716627e-02  6.09304085e-02 -3.15268673e-02 -2.24558059e-02\n",
       "                  -6.57034963e-02]\n",
       "                 [-7.83510432e-02 -7.77042471e-03  5.36566712e-02  8.85125995e-03\n",
       "                  -1.09600304e-02]]\n",
       "              \n",
       "                [[-6.30500019e-02  5.01118638e-02  5.38376160e-02 -2.75069140e-02\n",
       "                   5.25053367e-02]\n",
       "                 [ 3.19372453e-02  6.76084729e-03  6.93735778e-02  1.49704525e-02\n",
       "                   2.47359220e-02]\n",
       "                 [-3.92666198e-02  2.40850504e-02 -1.47265727e-02 -4.59061302e-02\n",
       "                   4.98994254e-02]\n",
       "                 [-2.45282315e-02  7.73533806e-02  4.85399105e-02  5.84835038e-02\n",
       "                   6.48889467e-02]\n",
       "                 [ 5.53981401e-02  7.05025066e-03 -5.65138906e-02 -5.04728667e-02\n",
       "                  -5.93205951e-02]]\n",
       "              \n",
       "                [[ 7.80571550e-02  2.95183789e-02  7.49481395e-02  7.50977620e-02\n",
       "                  -3.17472108e-02]\n",
       "                 [ 6.28231093e-02 -6.01811819e-02  6.00171350e-02  7.42448121e-02\n",
       "                   1.98325291e-02]\n",
       "                 [ 8.14728439e-02  4.47607040e-02 -5.04706278e-02 -7.63213038e-02\n",
       "                   4.06077579e-02]\n",
       "                 [ 7.98430368e-02  7.90600032e-02  2.02176496e-02  3.01452577e-02\n",
       "                   5.94280325e-02]\n",
       "                 [ 3.01985107e-02 -5.27582578e-02  2.49240696e-02  6.31332025e-02\n",
       "                   5.11543714e-02]]]\n",
       "              \n",
       "              \n",
       "               [[[ 5.87803610e-02  5.11439256e-02 -1.12844929e-02 -2.74076238e-02\n",
       "                  -2.54952256e-02]\n",
       "                 [ 6.40848130e-02  2.44273152e-02 -7.72342831e-02 -5.98408133e-02\n",
       "                   5.97804971e-02]\n",
       "                 [ 4.87273000e-02 -5.67472875e-02  3.00933309e-02  5.04514351e-02\n",
       "                   5.78202382e-02]\n",
       "                 [-4.62305956e-02  4.81760688e-02 -7.70719647e-02 -1.77244395e-02\n",
       "                  -5.67885265e-02]\n",
       "                 [-5.41829169e-02 -8.11009482e-02  5.04793189e-02 -4.83596958e-02\n",
       "                   6.47505522e-02]]\n",
       "              \n",
       "                [[ 3.61458883e-02  3.79313752e-02  5.90904951e-02  2.07317881e-02\n",
       "                   4.04335856e-02]\n",
       "                 [ 6.95497915e-02  2.49369368e-02 -6.78773969e-02  3.93501446e-02\n",
       "                  -2.17796564e-02]\n",
       "                 [-3.43015753e-02  3.70219052e-02 -5.04922085e-02 -5.33557720e-02\n",
       "                  -4.34472114e-02]\n",
       "                 [-2.48571523e-02  6.03794307e-02  6.88886195e-02 -1.09096309e-02\n",
       "                  -7.94152990e-02]\n",
       "                 [-1.61692575e-02 -7.77470618e-02  5.89406714e-02 -3.36530283e-02\n",
       "                  -1.70391314e-02]]\n",
       "              \n",
       "                [[ 6.54621348e-02 -7.23127350e-02 -5.23131005e-02 -8.09621736e-02\n",
       "                  -7.59786218e-02]\n",
       "                 [ 4.73195966e-03  1.32082216e-03 -5.38569363e-03  5.23372106e-02\n",
       "                   1.55233974e-02]\n",
       "                 [ 6.99807405e-02  5.57686714e-03  5.48859499e-02  4.90177535e-02\n",
       "                  -5.84467687e-02]\n",
       "                 [-4.84951846e-02  4.59301518e-03 -3.09897382e-02 -1.02022458e-02\n",
       "                   6.26123399e-02]\n",
       "                 [-2.12707352e-02 -1.35580106e-02  1.25090126e-02 -7.40573928e-02\n",
       "                   4.64513758e-03]]\n",
       "              \n",
       "                [[ 6.47185594e-02  4.51170728e-02  5.85317798e-02 -5.93481697e-02\n",
       "                  -1.16560264e-02]\n",
       "                 [-5.20364605e-02  3.83589119e-02  5.86594343e-02 -2.73295138e-02\n",
       "                   7.11249113e-02]\n",
       "                 [-2.78243199e-02  2.26239301e-02  6.36555180e-02 -7.05762878e-02\n",
       "                  -4.26529087e-02]\n",
       "                 [-1.37126744e-02  2.34610625e-02 -5.41898757e-02 -3.02970801e-02\n",
       "                  -1.77276805e-02]\n",
       "                 [-4.96289916e-02  5.42124771e-02 -6.61753304e-03 -3.27396579e-02\n",
       "                   9.35374666e-03]]\n",
       "              \n",
       "                [[ 4.20772284e-02 -4.65260521e-02  1.64026245e-02  2.36810856e-02\n",
       "                   3.62308919e-02]\n",
       "                 [ 7.17005925e-03 -7.90069252e-02 -4.49007004e-02  6.56892806e-02\n",
       "                   6.78115264e-02]\n",
       "                 [-6.06139079e-02  6.76662028e-02 -4.49736230e-02 -3.09397373e-02\n",
       "                   5.79265654e-02]\n",
       "                 [ 7.48611987e-02 -6.23943806e-02  3.99493612e-02 -2.00149417e-02\n",
       "                  -3.69700044e-02]\n",
       "                 [ 7.62169436e-02 -7.53694028e-02 -7.87575766e-02 -2.64688767e-02\n",
       "                   3.34722511e-02]]\n",
       "              \n",
       "                [[-7.18658417e-02 -1.76990461e-02  6.75824955e-02 -1.74894277e-02\n",
       "                   4.97832149e-02]\n",
       "                 [-6.64852113e-02  3.33953090e-02  6.19171225e-02  8.00500140e-02\n",
       "                   4.12547365e-02]\n",
       "                 [ 4.58306894e-02 -1.67973731e-02  5.83691038e-02  6.92980438e-02\n",
       "                  -3.96537259e-02]\n",
       "                 [-2.64880601e-02 -4.79279645e-02 -2.57988684e-02  7.81726465e-02\n",
       "                   1.99081562e-02]\n",
       "                 [ 4.54213023e-02 -3.50244269e-02 -6.21276870e-02  4.02821638e-02\n",
       "                   7.05318153e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [ 0.02033021  0.04464145  0.03729535  0.01910273  0.05996804  0.01589221\n",
       "               -0.05646405  0.05435165  0.05558667 -0.06226927 -0.02135489 -0.0108234\n",
       "               -0.08093588  0.01664229 -0.04445679 -0.06446783])),\n",
       "             ('fc1.weight',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [[ 0.00881025  0.02944019 -0.02478772 ...  0.02206214 -0.05024093\n",
       "                -0.03952261]\n",
       "               [-0.0286679  -0.00045091  0.01461441 ...  0.03696288 -0.02713848\n",
       "                 0.01876065]\n",
       "               [-0.0425133  -0.05827074  0.0088587  ...  0.02714065 -0.00096832\n",
       "                 0.01238867]\n",
       "               ...\n",
       "               [-0.05451984 -0.01606175 -0.02121263 ...  0.02901004  0.04343478\n",
       "                 0.02715748]\n",
       "               [-0.00311659  0.02525105 -0.0091098  ...  0.04422704 -0.03446654\n",
       "                 0.01247727]\n",
       "               [ 0.02841351  0.05340098 -0.0006504  ... -0.02753986 -0.0065631\n",
       "                -0.04503093]])),\n",
       "             ('fc1.bias',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [ 0.00608698  0.01021095  0.02410495 -0.01040829  0.05410184 -0.00624168\n",
       "                0.03936786  0.05585637  0.02721948 -0.05845521  0.06219999 -0.00944375\n",
       "               -0.00805008 -0.02121492 -0.0182265  -0.00177554 -0.03302971  0.00492275\n",
       "               -0.04085975  0.05952023  0.0438757   0.00821584  0.03755036 -0.04477447\n",
       "               -0.01991865  0.00273604 -0.0329359  -0.05917653 -0.03758728 -0.03104972\n",
       "                0.03020877 -0.0334444  -0.01532087 -0.05372503 -0.04120176 -0.04173708\n",
       "                0.02180605  0.0309506  -0.01078898 -0.03219511  0.04315355 -0.02727504\n",
       "               -0.01776082 -0.02196705  0.0085689  -0.01516853  0.01369429 -0.05312283\n",
       "               -0.05391549 -0.01242413  0.0104402  -0.02135101  0.02147534  0.01597638\n",
       "                0.05584814 -0.01825785  0.04849413 -0.04481857  0.01280174  0.01739381\n",
       "                0.0057375  -0.04858676  0.01409234 -0.03862623 -0.0485445   0.01910719\n",
       "               -0.02901545  0.05850448  0.00176712 -0.06130505  0.04928372 -0.03047482\n",
       "               -0.03280678 -0.01494923 -0.04897454 -0.01921636 -0.02762523  0.00234642\n",
       "                0.03904077 -0.01117828  0.00260685 -0.01976381 -0.05484276 -0.06174376\n",
       "                0.05081321 -0.06112661  0.04146727  0.00756621  0.05937904 -0.04338826\n",
       "                0.02996708  0.05297889  0.04435217 -0.03705463 -0.05012225  0.05888603\n",
       "                0.01101348  0.03178047 -0.0375248  -0.05490591 -0.05416549  0.04188479\n",
       "               -0.03668027 -0.03649095 -0.04171554 -0.05256096  0.01165436 -0.03894567\n",
       "               -0.05824077  0.06022883 -0.03101439  0.03831454  0.04739524 -0.02819267\n",
       "                0.02824814  0.04755232  0.00247868 -0.00329544  0.0003847   0.04023617])),\n",
       "             ('fc2.weight',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [[ 0.05178161 -0.04841973  0.06114998 ...  0.08815993 -0.04062474\n",
       "                -0.0139459 ]\n",
       "               [ 0.00771389 -0.08170166 -0.06129238 ... -0.06058913  0.03064266\n",
       "                 0.01310906]\n",
       "               [-0.08141487  0.04129969 -0.08200183 ...  0.0643245  -0.02676723\n",
       "                -0.05002303]\n",
       "               ...\n",
       "               [-0.03765994 -0.05657161 -0.01786201 ...  0.0316523  -0.02291494\n",
       "                 0.07529578]\n",
       "               [ 0.06241077  0.07037521  0.01136606 ... -0.05026975  0.05258187\n",
       "                -0.0487395 ]\n",
       "               [-0.02939167 -0.03990492 -0.00606507 ... -0.09090817 -0.05712991\n",
       "                -0.02647008]])),\n",
       "             ('fc2.bias',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [ 0.04654249 -0.04972672  0.03458181 -0.00814881 -0.00388504 -0.01840746\n",
       "               -0.00394229  0.01072421  0.04424978  0.05256698  0.08924256  0.06322733\n",
       "               -0.07680204 -0.07698181 -0.01164993 -0.01152979 -0.07506935 -0.04445357\n",
       "               -0.07112987  0.0315845   0.03521472  0.03932639  0.0903901  -0.05344177\n",
       "                0.02505603 -0.07998867 -0.08061109  0.04509184  0.06030515 -0.03813735\n",
       "                0.07483538 -0.02092483  0.05467362 -0.0830375   0.04369562 -0.05153575\n",
       "                0.0508653  -0.05227977 -0.01884045  0.01955582  0.02737799 -0.0134013\n",
       "                0.02325809 -0.00092985  0.00518431 -0.09113446  0.01365065  0.07124182\n",
       "               -0.03326441 -0.02089124  0.00428635  0.05764763  0.02362669  0.05960226\n",
       "               -0.03043249 -0.02608152 -0.02390546  0.03761734 -0.07328203 -0.03350669\n",
       "               -0.00668939  0.0127107   0.06374528 -0.01346001  0.01735898 -0.06737818\n",
       "                0.00258259 -0.04297764 -0.08681701  0.07661916  0.00586192  0.06861962\n",
       "                0.03923553 -0.08869904 -0.05682036 -0.06472486 -0.03061531  0.00537623\n",
       "                0.00183771  0.03723341  0.06202907 -0.02506054 -0.05312576  0.02997652])),\n",
       "             ('fc3.weight',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [[ 1.62297543e-02 -5.55116050e-02 -1.05817892e-01  3.23510729e-02\n",
       "                -4.14403155e-03 -4.27725762e-02 -8.94396976e-02 -5.37813753e-02\n",
       "                -5.07000238e-02 -4.23114337e-02  3.75022218e-02  9.84477699e-02\n",
       "                -1.03487983e-01  1.03861272e-01  4.52267006e-02 -8.05312619e-02\n",
       "                -7.10833222e-02  3.28823999e-02 -2.87027042e-02 -3.68905365e-02\n",
       "                -8.69534910e-02 -1.59767475e-02 -9.66966003e-02  5.72090968e-02\n",
       "                -5.62221259e-02 -7.55490735e-02  7.04988614e-02 -9.16514695e-02\n",
       "                -1.04222529e-01  5.47648966e-02  1.00016683e-01 -1.53520303e-02\n",
       "                -4.90493961e-02 -7.39467293e-02 -5.83513686e-03 -1.03754997e-01\n",
       "                 1.19095203e-02 -8.82974043e-02  2.80588288e-02  6.11664429e-02\n",
       "                 6.58003613e-03  7.72417784e-02  1.07598752e-01  8.43634307e-02\n",
       "                -4.22614999e-02  3.30919400e-02  4.47852351e-02  1.51476283e-02\n",
       "                -8.66502896e-02  1.48588251e-02  5.55849262e-02 -1.07053094e-01\n",
       "                 8.51040781e-02 -4.94544283e-02 -1.11035276e-02  1.71381235e-02\n",
       "                 7.13844821e-02 -5.01642376e-03  3.17892171e-02 -4.88710366e-02\n",
       "                 1.65934376e-02  9.03637931e-02  6.32029846e-02 -6.29729927e-02\n",
       "                 1.00459151e-01  1.91100072e-02  6.87712952e-02  7.94147477e-02\n",
       "                 1.98334064e-02 -2.33482718e-02  9.30054262e-02  5.06475940e-02\n",
       "                 5.67097664e-02  3.70490886e-02  2.57116221e-02  1.01604182e-02\n",
       "                 1.00311071e-01  9.06714946e-02 -1.06698908e-01  7.27598816e-02\n",
       "                -1.79773103e-02  8.02192539e-02  9.69176218e-02 -1.03419736e-01]\n",
       "               [ 6.13397844e-02 -8.83476511e-02 -3.35450582e-02 -8.84254575e-02\n",
       "                -5.89002781e-02  2.01181369e-03  3.14578973e-02  8.54213815e-03\n",
       "                -1.06365152e-01 -5.37257595e-03  9.02923122e-02 -4.64739352e-02\n",
       "                 1.04035176e-01 -1.45608913e-02 -4.88320785e-03  5.72988577e-02\n",
       "                -6.67037889e-02 -2.17930749e-02  2.94558629e-02 -5.84625341e-02\n",
       "                -4.38076444e-02  3.99652384e-02  5.44641279e-02  7.96993077e-02\n",
       "                 1.40889664e-04  1.31339282e-02  3.87809463e-02  1.01951420e-01\n",
       "                -5.10721765e-02  8.38444978e-02 -9.84210242e-03  4.96677647e-04\n",
       "                 1.17121814e-02  4.88231070e-02  2.65856516e-02 -4.32784343e-03\n",
       "                -3.11379544e-02 -5.89488074e-02  5.98672964e-02 -9.18608755e-02\n",
       "                -3.92206796e-02 -7.64684100e-03  4.69428152e-02  5.46822790e-03\n",
       "                -8.11049938e-02  4.39958796e-02 -5.47390655e-02 -5.79149872e-02\n",
       "                 5.08188568e-02 -1.91924050e-02  8.67826194e-02 -9.73679051e-02\n",
       "                 4.23406996e-02 -2.60182433e-02 -7.82294646e-02  9.26356241e-02\n",
       "                -9.13598537e-02 -1.05346754e-01  6.81489110e-02 -3.13693844e-02\n",
       "                 9.29014608e-02  8.60582963e-02  9.19419080e-02 -5.99967539e-02\n",
       "                 4.81147543e-02  4.32215966e-05 -2.52116919e-02 -9.89382416e-02\n",
       "                -1.06167212e-01  7.08258003e-02  5.96616864e-02 -6.23987056e-02\n",
       "                 3.31259519e-02  8.08166489e-02  6.80365041e-02 -2.25785952e-02\n",
       "                 7.98083618e-02  7.72185624e-02  7.46760592e-02  5.96157555e-03\n",
       "                 1.58597121e-03  5.61662875e-02  7.37364069e-02  9.09870118e-02]\n",
       "               [-1.07780330e-01 -1.08541094e-01  4.46331985e-02 -9.91000682e-02\n",
       "                 5.39429337e-02 -2.27440801e-02  2.47566239e-03 -9.30286422e-02\n",
       "                -6.35310784e-02  7.17572644e-02  7.68294111e-02  6.41857907e-02\n",
       "                 2.59009227e-02  3.59291919e-02 -6.81419522e-02 -7.92210773e-02\n",
       "                -9.38372966e-03  8.40391740e-02 -2.99085509e-02  9.39437747e-02\n",
       "                -7.38114417e-02 -7.26706013e-02  1.14402482e-02 -5.39160743e-02\n",
       "                 6.47946894e-02  7.40847588e-02 -2.51901913e-02 -1.07823268e-01\n",
       "                 4.76055518e-02  4.78588715e-02 -4.36664931e-02 -2.21737716e-02\n",
       "                -7.33703673e-02  1.99147109e-02 -9.58940908e-02  6.51488900e-02\n",
       "                 1.07910290e-01  2.42158137e-02 -8.00881386e-02  1.85823869e-02\n",
       "                -6.38234317e-02  5.09463735e-02 -2.73123030e-02  6.32045940e-02\n",
       "                 2.32609976e-02 -9.91014615e-02  9.74448100e-02  6.85777813e-02\n",
       "                 9.05497298e-02  1.05868302e-01 -6.44475743e-02 -1.07782461e-01\n",
       "                 6.55994937e-02  5.98068908e-02 -2.46637277e-02 -1.44074894e-02\n",
       "                -6.02592304e-02 -4.05212305e-02 -6.45964146e-02 -4.91422415e-02\n",
       "                -6.41791672e-02 -4.54447716e-02  4.29601483e-02 -8.40643272e-02\n",
       "                 7.73457158e-03  2.15289332e-02 -3.33789103e-02  7.11762533e-02\n",
       "                 5.62257320e-02  5.54077849e-02  4.60840017e-03 -1.47794187e-02\n",
       "                 1.05327584e-01 -1.17453355e-02 -1.72772706e-02 -4.78834808e-02\n",
       "                -1.08024411e-01 -3.21057364e-02  8.83367658e-02 -8.03544372e-02\n",
       "                -4.95058075e-02 -7.61735290e-02 -7.51573518e-02 -5.95052242e-02]\n",
       "               [ 1.25556979e-02 -7.86546841e-02  6.59006415e-03  9.83658880e-02\n",
       "                 9.59743783e-02  7.27536529e-02  9.17707803e-04  7.54570886e-02\n",
       "                -2.97355875e-02 -7.40412101e-02  7.53439292e-02  7.44426027e-02\n",
       "                -3.15441452e-02  7.57242516e-02 -7.00568259e-02 -5.96863590e-02\n",
       "                -1.04784705e-01 -7.44042024e-02  3.87915336e-02  5.79375029e-02\n",
       "                 1.15554491e-02 -2.97330637e-02 -1.54592318e-03  9.38425809e-02\n",
       "                 4.15244810e-02 -6.93995133e-02  1.66778918e-02  4.43776809e-02\n",
       "                 6.41819015e-02 -6.10978305e-02 -4.47798113e-04  1.07713461e-01\n",
       "                -4.13434021e-02 -6.45840168e-02 -6.08408675e-02  5.62405065e-02\n",
       "                -4.51652035e-02  8.30210894e-02  4.40186262e-02  9.78298411e-02\n",
       "                 4.85177450e-02  1.48959076e-02  9.22156125e-02 -7.79231116e-02\n",
       "                -5.12704141e-02  8.28231052e-02 -9.81338322e-02 -7.98406973e-02\n",
       "                -4.97205481e-02  3.57798226e-02 -4.73944508e-02 -8.69963318e-02\n",
       "                 1.08036332e-01 -4.20462899e-02 -8.30647796e-02 -5.01652882e-02\n",
       "                -7.11445138e-02 -2.78455559e-02  5.76594062e-02  3.51336189e-02\n",
       "                 5.54286987e-02 -1.77884381e-02 -5.84515333e-02  8.05767849e-02\n",
       "                 4.67757322e-02  1.39499884e-02 -5.12113608e-02 -2.98053939e-02\n",
       "                -3.36474739e-02 -6.40547182e-03 -3.54883522e-02  7.39381686e-02\n",
       "                -5.28087132e-02 -1.81200206e-02  6.28626421e-02 -3.50498408e-02\n",
       "                 9.55674499e-02  1.06629722e-01  4.04237285e-02  2.94539630e-02\n",
       "                -1.36908274e-02  8.36465210e-02  7.74233267e-02 -8.58818963e-02]\n",
       "               [ 8.55226740e-02 -6.99683353e-02 -1.12221367e-03  9.59668532e-02\n",
       "                 6.12165183e-02 -2.98388358e-02  6.58338442e-02  1.08278252e-01\n",
       "                -2.12922357e-02  1.85840372e-02 -1.05664082e-01 -2.10456401e-02\n",
       "                 5.17545901e-02  4.61494662e-02 -6.86330721e-02 -6.31080344e-02\n",
       "                -6.98401332e-02  7.04195276e-02 -5.77981863e-03 -1.69967264e-02\n",
       "                 1.14442147e-02 -9.64507833e-02 -7.17911273e-02  6.57578260e-02\n",
       "                -5.93464896e-02  2.57660151e-02  1.75961189e-03  3.64008434e-02\n",
       "                 6.28207102e-02  9.89738926e-02 -1.95515882e-02 -7.44963065e-02\n",
       "                -9.13151130e-02  8.62157717e-02 -3.68512161e-02 -1.00496583e-01\n",
       "                 1.91123616e-02  3.44509035e-02 -2.39222627e-02  1.02178663e-01\n",
       "                 2.44181212e-02 -6.54172078e-02 -4.54406478e-02  7.00886101e-02\n",
       "                -4.54050489e-03  2.03769337e-02 -5.77520020e-02  6.80967122e-02\n",
       "                 8.13300386e-02  9.30921659e-02 -9.31529701e-02  3.11806165e-02\n",
       "                -5.39736301e-02  2.32867897e-02  3.44385616e-02 -5.26252128e-02\n",
       "                 3.91619392e-02  7.56973634e-03 -5.62724359e-02  1.02596171e-01\n",
       "                -6.35553896e-03  1.34868287e-02 -6.05042651e-02  6.23870641e-02\n",
       "                -4.97368053e-02  1.09759318e-02 -7.44059235e-02 -2.71265525e-02\n",
       "                 4.58981618e-02 -6.54012710e-02 -9.40014720e-02 -4.27741483e-02\n",
       "                -2.57609691e-02 -1.64521057e-02  2.47143768e-02  3.44595648e-02\n",
       "                 5.09664826e-02  8.40031952e-02  7.92844296e-02 -6.68379888e-02\n",
       "                 4.25252505e-02  2.46837325e-02 -7.32089430e-02 -9.16910246e-02]\n",
       "               [-9.13588628e-02  9.93555784e-02  2.32485496e-02 -4.51870039e-02\n",
       "                 4.93393056e-02 -9.96811762e-02 -1.57292672e-02  7.42033124e-02\n",
       "                -7.80529976e-02  5.58524989e-02  5.01780622e-02 -7.29773566e-02\n",
       "                 2.33967360e-02 -4.60765250e-02 -7.08956718e-02 -6.83502555e-02\n",
       "                 7.38008022e-02 -7.37642720e-02  4.53832112e-02  8.03860798e-02\n",
       "                 3.33357006e-02  9.92746204e-02 -4.08002501e-03 -2.23005489e-02\n",
       "                 9.49985441e-03 -1.05889253e-01  3.37630510e-02 -4.57774336e-03\n",
       "                -4.40848581e-02  5.56406602e-02  8.25361460e-02 -6.61282614e-03\n",
       "                -1.05918795e-02  3.60586606e-02  6.72903657e-02 -5.75785153e-02\n",
       "                -1.01438016e-01  8.39008465e-02 -8.54482278e-02  3.00302040e-02\n",
       "                 5.10997251e-02 -1.02037698e-01 -1.64522622e-02  9.54171941e-02\n",
       "                 6.18644767e-02  5.83128519e-02  6.68601394e-02  3.56544629e-02\n",
       "                 1.07952900e-01 -2.53624795e-03  5.05088530e-02 -3.01999301e-02\n",
       "                 4.61365506e-02  5.44531234e-02 -1.97604764e-02 -4.32642065e-02\n",
       "                -9.84822866e-04  7.18963519e-02  7.64645077e-03  4.66961674e-02\n",
       "                -8.03544596e-02  3.68612967e-02  2.30678078e-02 -6.30327314e-02\n",
       "                -1.03047274e-01  1.47018465e-03 -7.64444917e-02  3.40590477e-02\n",
       "                -7.87923709e-02 -5.77997081e-02  5.66068944e-03  4.71526943e-02\n",
       "                 2.55060624e-02  2.58278754e-02  3.84316221e-02 -9.24446732e-02\n",
       "                 8.46635252e-02  7.60789216e-02 -5.39503060e-02 -6.88193068e-02\n",
       "                -1.91058964e-02 -4.22650911e-02 -1.94799472e-02  7.35663697e-02]\n",
       "               [-3.28264572e-02  3.99163086e-03 -1.72999147e-02  7.45695680e-02\n",
       "                -1.04165025e-01  9.44740996e-02  1.00409895e-01 -9.09159034e-02\n",
       "                -4.57650498e-02  7.99516439e-02 -1.40996445e-02 -1.00734040e-01\n",
       "                -7.50177726e-02  8.69089644e-03  6.48329034e-02 -5.50192334e-02\n",
       "                -9.01960060e-02  6.82826564e-02  8.98735449e-02  4.54166904e-02\n",
       "                -6.55992925e-02 -4.47602868e-02  8.65461156e-02 -2.71504596e-02\n",
       "                 2.18758769e-02  1.36541612e-02  8.57058540e-02  3.30950618e-02\n",
       "                -2.38348311e-03  4.95186821e-02 -4.55002077e-02  9.95589793e-02\n",
       "                 7.64166117e-02  3.74874435e-02 -1.39022926e-02 -2.45488919e-02\n",
       "                 6.94640875e-02  6.60512075e-02 -2.07983935e-03 -8.13295543e-02\n",
       "                 8.64382908e-02  4.04777229e-02 -1.51772453e-02 -9.97902006e-02\n",
       "                 5.93119040e-02 -9.90954936e-02  4.57465276e-02  5.62338866e-02\n",
       "                 9.66297835e-02  6.68256208e-02  6.24957867e-02 -6.00866936e-02\n",
       "                 3.32975388e-02  3.91922332e-02 -3.40660065e-02  1.81407407e-02\n",
       "                 4.62749153e-02  8.05359483e-02 -6.58213571e-02 -6.33313060e-02\n",
       "                -8.68370384e-03 -8.77430141e-02  2.93201883e-03 -8.69102031e-02\n",
       "                 2.60684621e-02  4.19663489e-02 -5.96637800e-02 -3.06366719e-02\n",
       "                -3.84976715e-02 -9.85135660e-02 -9.98665094e-02  7.91232809e-02\n",
       "                -7.05927312e-02  5.72205037e-02  7.89947584e-02 -8.09757486e-02\n",
       "                 2.71992479e-02 -4.40808646e-02  5.96595109e-02 -1.41330715e-02\n",
       "                -3.13242637e-02 -1.01761833e-01 -5.96987009e-02 -4.51142415e-02]\n",
       "               [-5.00322431e-02  9.24429819e-02  5.06979190e-02  6.63093850e-02\n",
       "                 8.32944587e-02 -7.16946945e-02 -3.68358828e-02  9.25829783e-02\n",
       "                -3.87956314e-02 -9.75462794e-02  9.49519053e-02  9.28992853e-02\n",
       "                -8.69093314e-02  1.61846336e-02 -8.20684358e-02  9.05172974e-02\n",
       "                 2.34083254e-02 -7.78381154e-02 -5.35881473e-03  1.29458504e-02\n",
       "                 7.38012865e-02  9.04720202e-02  1.07517406e-01  6.52920976e-02\n",
       "                -1.03989832e-01 -9.32577159e-03 -6.30758628e-02  7.90760517e-02\n",
       "                 9.50760171e-02  7.89503306e-02 -4.00538184e-02 -5.23211919e-02\n",
       "                -6.68473169e-02 -7.32823834e-02  6.13513328e-02 -5.57506550e-03\n",
       "                -2.67767347e-02 -1.11866808e-02 -1.54576718e-03  7.77745917e-02\n",
       "                -9.68124643e-02  4.89482321e-02  2.96360329e-02  1.63853951e-02\n",
       "                 8.38045552e-02 -1.97910555e-02  6.19120300e-02  8.84968117e-02\n",
       "                 6.20395765e-02 -9.18106437e-02 -1.51096098e-03  3.35359648e-02\n",
       "                 1.77188125e-02 -8.39203969e-03 -3.59883606e-02 -1.06645487e-01\n",
       "                 3.35137360e-02 -2.15231329e-02  2.58490653e-03 -9.49218869e-02\n",
       "                 1.75457709e-02  5.35669960e-02  6.56620488e-02  7.23134354e-02\n",
       "                 3.50727178e-02 -1.01995505e-01  1.10803107e-02 -4.58155423e-02\n",
       "                -6.27654865e-02  1.14888810e-02  8.56900886e-02  8.78973007e-02\n",
       "                 9.61461216e-02  1.61729157e-02 -5.59082739e-02  3.95668559e-02\n",
       "                 3.82322781e-02 -9.37391371e-02 -3.73036321e-03  5.17089106e-02\n",
       "                -1.29569061e-02 -7.84360394e-02 -8.21825415e-02  3.43239568e-02]\n",
       "               [ 6.57233596e-02 -4.31537777e-02  6.58373013e-02  4.70190495e-02\n",
       "                -9.55328271e-02 -2.28057466e-02  3.11013404e-02 -3.97469997e-02\n",
       "                 5.14414534e-02 -6.21887622e-03  9.02324915e-02 -6.01266138e-02\n",
       "                -6.01727478e-02  8.31062049e-02 -6.89025223e-02 -8.71112868e-02\n",
       "                -4.40181196e-02  6.57793283e-02  7.82293975e-02 -6.66035190e-02\n",
       "                 1.58618446e-02  4.63614386e-04  3.02635715e-03  4.64564264e-02\n",
       "                 7.77620226e-02  6.66032434e-02 -8.08414742e-02  4.92023043e-02\n",
       "                -5.96260466e-02  4.93422709e-02  1.03159146e-02 -9.79198441e-02\n",
       "                 5.36611937e-02  8.12805071e-02  1.00985669e-01 -1.00766785e-01\n",
       "                 9.83756483e-02 -4.08217125e-02  1.68233854e-03 -9.09669474e-02\n",
       "                 3.28792147e-02 -1.81121118e-02  2.96429265e-02 -2.85238605e-02\n",
       "                -5.23408316e-02 -1.68336090e-02  1.94196720e-02  7.65961111e-02\n",
       "                -2.34895535e-02 -4.28428240e-02 -5.94914146e-03  1.20391073e-02\n",
       "                -1.01470955e-01 -4.39591222e-02 -1.84732978e-03  1.06856093e-01\n",
       "                 2.45959889e-02 -1.04430296e-01  1.43252732e-02  4.03334238e-02\n",
       "                 5.48886843e-02  3.39672193e-02  4.84071597e-02 -8.86880234e-02\n",
       "                 1.95308421e-02  1.04947966e-02  5.21487631e-02  4.03746180e-02\n",
       "                -3.66485305e-02 -3.84624489e-02  3.99638340e-02  7.05562681e-02\n",
       "                -9.32718962e-02 -8.92337635e-02  5.19064954e-03  5.95593080e-02\n",
       "                 8.25241953e-02 -3.76357213e-02  1.01227894e-01  1.25771724e-02\n",
       "                 1.20349713e-02 -9.08764079e-02  6.38763681e-02  9.43043754e-02]\n",
       "               [ 8.06579366e-02 -6.60761297e-02 -5.39683886e-02 -3.32160890e-02\n",
       "                 7.43841305e-02  3.39117721e-02 -3.20877763e-04  9.83160734e-02\n",
       "                 5.29387444e-02  4.95028533e-02  3.92616764e-02  9.65702906e-03\n",
       "                 3.85811590e-02  6.69296905e-02  5.09779155e-02 -4.10498381e-02\n",
       "                -8.96894708e-02  7.36751184e-02  7.43733272e-02 -3.42623554e-02\n",
       "                -6.05295226e-02 -1.59307029e-02 -3.62254344e-02 -2.08019447e-02\n",
       "                -7.04019442e-02  6.22556843e-02  1.33037325e-02  9.20578763e-02\n",
       "                -8.71038735e-02 -9.39129218e-02  4.32149768e-02  1.34273488e-02\n",
       "                -7.08674192e-02  3.79934087e-02 -2.07486302e-02 -4.62727845e-02\n",
       "                 9.67179015e-02 -8.64036568e-03 -8.30541924e-02  7.28379264e-02\n",
       "                 6.81626424e-02 -2.34313868e-02 -3.45707871e-02  9.41907987e-02\n",
       "                -3.00872251e-02 -5.55081442e-02  6.14490639e-03 -7.62574002e-02\n",
       "                -8.69617090e-02  9.47117954e-02  5.37940301e-02 -1.05098486e-01\n",
       "                 3.24822729e-03 -6.56018630e-02 -5.60822375e-02 -9.45563540e-02\n",
       "                 9.93432179e-02  9.86272320e-02 -9.14470255e-02  8.62686262e-02\n",
       "                -9.54495147e-02 -3.04027051e-02  4.07767482e-02  6.07993230e-02\n",
       "                -2.73805372e-02 -3.97995077e-02 -4.43316475e-02 -9.71892923e-02\n",
       "                 7.98868537e-02 -8.20692033e-02  9.45842266e-02 -7.24497139e-02\n",
       "                 3.33485492e-02 -1.07760951e-02  8.00960734e-02 -6.69415519e-02\n",
       "                -4.86020148e-02  4.29400010e-03 -2.60702707e-02  6.74275756e-02\n",
       "                -8.24941322e-02  4.03458700e-02  8.28538369e-03  1.03177369e-01]])),\n",
       "             ('fc3.bias',\n",
       "              Tensor(<class 'jaxlib._jax.ArrayImpl'> [ 0.0423394  -0.0496862  -0.00994985  0.05534628 -0.06091116 -0.10226545\n",
       "               -0.04402063 -0.07692246  0.04418553  0.00122255]))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTrYAn1LsyPE"
   },
   "source": [
    "Per-Epoch Activity\n",
    "==================\n",
    "\n",
    "There are a couple of things we'll want to do once per epoch:\n",
    "\n",
    "-   Perform validation by checking our relative loss on a set of data\n",
    "    that was not used for training, and report this\n",
    "-   Save a copy of the model\n",
    "\n",
    "Here, we'll do our reporting in TensorBoard. This will require going to\n",
    "the command line to start TensorBoard, and opening it in another browser\n",
    "tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfcNC9UwsyPE",
    "outputId": "ea17f7c0-a586-4519-acb5-d49e2923c14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5943: UserWarning: Explicitly requested dtype int64 requested in arange is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return _arange(start, stop=stop, step=step, dtype=dtype,\n",
      "/Users/hanq/git/qihqi/torchax/torchax/ops/mappings.py:83: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:209.)\n",
      "  res = torch.from_numpy(numpy.asarray(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 0.9756997779496014\n",
      "  batch 2000 loss: 0.6890982975531369\n",
      "LOSS train 0.6890982975531369 valid Tensor(<class 'jaxlib._jax.ArrayImpl'> 0.65348643)\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.5893321642498486\n",
      "  batch 2000 loss: 0.5676386482019443\n",
      "LOSS train 0.5676386482019443 valid Tensor(<class 'jaxlib._jax.ArrayImpl'> 0.511011)\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to('jax')\n",
    "            vlabels = vlabels.to('jax')\n",
    "            model.load_state_dict(weights) # put the trained weight back to test it\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            if i > 1000:\n",
    "                break\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model checkpoint\n",
    "\n",
    "Currently `torch.save` (which is based on Pickle) are not able to save tensors on 'jax' device. \n",
    "Because JAX arrays cannot be pickled.\n",
    "\n",
    "So now we have 2 strategies for saving:\n",
    "1. convert the tensors on jax devices to plain JAX arrays; then use flax.checkpoint to save the data. You will get an JAX-style checkpoint (directory) if you do so.\n",
    "2. convert the tensors from jax devices to CPU torch.Tensor, then use `torch.save`; you will get a regular pickle based checkpoint if you do so.\n",
    "\n",
    "We recommend 1. and we have provided wrapper in `torchax.save_checkpoint` that does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import orbax.checkpoint as ocp\n",
    "ckpt_dir = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\n",
    "model_path = ckpt_dir / 'state'\n",
    "torchax.save_checkpoint(weights, model_path, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/my-checkpoints/\n",
      "/tmp/my-checkpoints/torch_checkpoint.pkl\n",
      "/tmp/my-checkpoints/state\n",
      "/tmp/my-checkpoints/state/checkpoint_1\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_sharding\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_CHECKPOINT_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas/process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d/64caa9fda45b7e010ab72cc60511e3b4\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/a37364fedb7049546fd77e7acc4bb7a9\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/e52cc72492daf25e6f82c2410145c041\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/40aba937d11193a6432d60078b74d341\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/fba2ca4921e7c7379d08a30a1978ab38\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/16d4ae29aca4975149d33bddfaf7d3cb\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/c5ffe437b0783f5e80283f4edc98ee5d\n"
     ]
    }
   ],
   "source": [
    "!find /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also produce a torch pickle based checkpoint by moving the state_dict to CPU\n",
    "\n",
    "You can do so with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpu_state_dict = jax.tree.map(lambda a: a.jax(), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cpu_state_dict, ckpt_dir / 'torch_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mstate\u001b[m\u001b[m                torch_checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
