{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2qVWNg4syO_"
   },
   "source": [
    "\n",
    "\n",
    "Training a PyTorch model with JAX\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb039419"
   },
   "source": [
    "\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "This tutorial notebook is adapted from https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "It will keep the most PyTorch code unchanged (especially the model definition),\n",
    "and will replace the standard PyTorch train loop (`loss.backward()` + `optimizer.step()` pattern)\n",
    "with a JAX train loop (`jax.grad` followed by `optax.apply_updates`).\n",
    "\n",
    "The rest of the tutorial, such as data loading, print loss etc. are kept as close to the original as possible.\n",
    "\n",
    "Dataset and DataLoader\n",
    "----------------------\n",
    "\n",
    "The `Dataset` and `DataLoader` classes encapsulate the process of\n",
    "pulling your data from storage and exposing it to your training loop in\n",
    "batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single\n",
    "instances of data.\n",
    "\n",
    "The `DataLoader` pulls instances of data from the `Dataset` (either\n",
    "automatically or with a sampler that you define), collects them in\n",
    "batches, and returns them for consumption by your training loop. The\n",
    "`DataLoader` works with all kinds of datasets, regardless of the type of\n",
    "data they contain.\n",
    "\n",
    "For this tutorial, we'll be using the Fashion-MNIST dataset provided by\n",
    "TorchVision. We use `torchvision.transforms.Normalize()` to zero-center\n",
    "and normalize the distribution of the image tile content, and download\n",
    "both training and validation data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: torch in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.0.6)\n",
      "Requirement already satisfied: jax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.7.2)\n",
      "Requirement already satisfied: optax in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (0.2.6)\n",
      "Requirement already satisfied: tensorboard in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.7.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from jax) (1.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (2.3.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from optax) (0.1.91)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (6.32.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: toolz>=1.0.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Optional: install dependencies\n",
    "!pip install matplotlib torch torchax jax optax tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GyJEP81WsyO9"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48h6g79csyPB",
    "outputId": "0563dbdc-b9d9-4636-a13c-5aee818f879a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uV8TxRTsyPC"
   },
   "source": [
    "As always, let's visualize the data as a sanity check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "FObQHGljsyPC",
    "outputId": "46d0be39-7817-4dd2-bb0a-da8c995cbcd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coat  Pullover  Bag  Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwJeNNVpsyPC"
   },
   "source": [
    "The Model\n",
    "=========\n",
    "\n",
    "The model we'll use in this example is a variant of LeNet-5 - it should\n",
    "be familiar if you've watched the previous videos in this series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MXyOntrgsyPC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0147, -0.0986, -0.0197, -0.0880, -0.0228, -0.0379,  0.0820, -0.0990,\n",
       "         -0.0172, -0.0708],\n",
       "        [ 0.0056, -0.0992, -0.0195, -0.0978, -0.0150, -0.0213,  0.0677, -0.0953,\n",
       "         -0.0219, -0.0732],\n",
       "        [ 0.0166, -0.1112, -0.0197, -0.1034, -0.0170, -0.0254,  0.0676, -0.1010,\n",
       "         -0.0152, -0.0621],\n",
       "        [ 0.0213, -0.1105, -0.0365, -0.1026, -0.0205, -0.0142,  0.0628, -0.1015,\n",
       "         -0.0243, -0.0444]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUzquGuBsyPD"
   },
   "source": [
    "Loss Function\n",
    "=============\n",
    "\n",
    "For this example, we'll be using a cross-entropy loss. For demonstration\n",
    "purposes, we'll create batches of dummy output and label values, run\n",
    "them through the loss function, and examine the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j5HHPvrsyPD",
    "outputId": "b5a6be6c-e0b6-4206-cc60-27a5449cf0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0188, 0.1162, 0.7957, 0.0443, 0.0191, 0.1256, 0.8698, 0.1108, 0.4106,\n",
      "         0.8890],\n",
      "        [0.6096, 0.3726, 0.2039, 0.0199, 0.5399, 0.1214, 0.0381, 0.5662, 0.7744,\n",
      "         0.9680],\n",
      "        [0.2135, 0.7618, 0.2944, 0.8146, 0.8824, 0.8905, 0.8575, 0.8906, 0.0485,\n",
      "         0.0220],\n",
      "        [0.4641, 0.1055, 0.9248, 0.7413, 0.3650, 0.5146, 0.3190, 0.7306, 0.5251,\n",
      "         0.5850]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.370086431503296\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move model to 'jax' device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
      "WARNING:root:Duplicate op registration for aten.__and__\n"
     ]
    }
   ],
   "source": [
    "import torchax\n",
    "torchax.enable_globally()\n",
    "model.to('jax')\n",
    "images = images.to('jax')\n",
    "dummy_labels = dummy_labels.to('jax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiOna1CzsyPD"
   },
   "source": [
    "Optimizer\n",
    "=========\n",
    "\n",
    "For this example, we'll be using simple [optax](https://optax.readthedocs.io/en/latest/getting_started.html)\n",
    "optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tzcmT1YIsyPD"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "start_learning_rate = 1e-3\n",
    "optimizer = optax.adam(start_learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x14234ca40>, update=<function chain.<locals>.update_fn at 0x14234cae0>)\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ubeUOe6syPD"
   },
   "source": [
    "The Training Loop\n",
    "=================\n",
    "\n",
    "Below, we have a function that performs one training epoch.\n",
    "\n",
    "First, let's articulate what the training step does.\n",
    "\n",
    "At each training step, we first evaluate the model. the Model is a\n",
    "function that maps the `(weights, input data)` to `prediction`.\n",
    "\n",
    "$$ model: (weights, input) \\mapsto pred $$\n",
    "\n",
    "In PyTorch, we can use [torch.func.functional_call](https://docs.pytorch.org/docs/stable/generated/torch.func.functional_call.html) to call a model \n",
    "with weights passed in as a paramter.\n",
    "\n",
    "The loss is a function that takes the prediction, the label to a real number\n",
    "representing the loss:\n",
    "\n",
    "$$ loss: (pred, label) \\mapsto loss $$\n",
    "\n",
    "To train the model, we a glorified Gradient Descent (in this case Adam), so\n",
    "we need to have another function that represent the gradient of the \n",
    "loss with respect of weights.\n",
    "\n",
    "$$ \\frac {d loss} {d weights}$$\n",
    "\n",
    "Finally, the `train_step` itself is a function that takes (weights, optimizer_state, input_data) to\n",
    "(updated weights, and updated optimizer_states).\n",
    "\n",
    "We can spell out the individual components of a train loop, and use Python to assemble them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()\n",
    "\n",
    "def run_model_and_loss(weights, inputs, labels):\n",
    "    # First call the model with passed in weights\n",
    "    output = torch.func.functional_call(model, weights, args=(inputs, ))\n",
    "    loss = loss_fn(output, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(<class 'jaxlib._jax.ArrayImpl'> 2.3484843)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_and_loss(model.state_dict(), images, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the gradient function of it. In JAX, one would use `jax.jit`. \n",
    "However, `jax.jit` need to take a JAX function (function that takes jax.Array as inputs and outputs) as \n",
    "argument, and here `run_model_and_loss` takes torch.Tensor as inputs / outputs.\n",
    "\n",
    "One way to solve this issue is to use `jax_view` from the [torchax.interop module](https://github.com/google/torchax/blob/main/torchax/interop.py)\n",
    "\n",
    "`jax_view` converts a torch function to a jax function.\n",
    "\n",
    "`torchax` has common JAX functions wrapped in the [so they work with torch-functions as well.\n",
    "in this case, we will use `jax_value_and_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchax.interop import jax_view\n",
    "import jax\n",
    "\n",
    "grad_fn_jax = jax.grad( jax_view(run_model_and_loss))\n",
    "\n",
    "grad_fn_jax(jax_view(weights), jax_view(images), jax_view(dummy_labels)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above `grad_fn_jax` is the gradient of `jax_view(run_model_and_loss)` and is a jax function.\n",
    "\n",
    "if instead we wish to make a it into a torch function, we can use `torch_view` on it and it will\n",
    "become a function that takes torch tensors and returns torch tensors.\n",
    "\n",
    "In fact, the pattern of calling, `jax_view` + `jax.value_and_grad` + `torch_view` is common enough that\n",
    "we provided this very wraper as `torchax.interop.jax_value_and_grad` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = torchax.interop.jax_value_and_grad(run_model_and_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assemble the train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ei-0yCbisyPD"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "from torchax.interop import call_jax\n",
    "\n",
    "# Initialize optimizer, we need to call optimizer.init, but\n",
    "# it is a JAX-function (function that takes jax arrays as input),\n",
    "# so we use call_jax to pass it torch values:\n",
    "\n",
    "opt_state = call_jax(optimizer.init, weights)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    global weights\n",
    "    global opt_state\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('jax')\n",
    "        labels = labels.to('jax')\n",
    "\n",
    "        # compute gradients\n",
    "        loss, gradients = grad_fn(weights, inputs, labels)\n",
    "        # compute updates\n",
    "        updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
    "        #apply updates\n",
    "        weights = call_jax(optax.apply_updates, weights, updates)\n",
    "        \n",
    "        # Gather data and report\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        if i > 2000: \n",
    "            break\n",
    "            # NOTE: make it run faster for CI\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will work, however, the grad / optimizer update / apply update is pretty \n",
    "standard; so we have a helper to do exactly that [make_train_step](https://github.com/google/torchax/blob/f41e3de8526f9d4e8410bfb84660faaaf0b3ba4a/torchax/train.py#L28)\n",
    "\n",
    "Now let's use that instead.\n",
    "\n",
    "Having a variable for the function of one training step also allows us to compile it with `jax.jit`.\n",
    "Here we use `interop.jax_jit` which just wraps `jax.jit` with `torch_view` and pass kwargs verbatim to the \n",
    "underlying `jax.jit` as below.\n",
    "\n",
    "We can optionally donate the weight and optmizer state, so XLA can issue in-place updates for those 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from torchax.train import make_train_step\n",
    "\n",
    "\n",
    "# the calling convention to make_train_step is the model_fn\n",
    "# takes weights (trainable params) and buffers (non-trainable params)\n",
    "# separately. because jax.jit will compute gradients wrt the first arg.\n",
    "def model_fn(weights, buffers, data):\n",
    "    return torch.func.functional_call(model, (weights, buffers), data)\n",
    "\n",
    "\n",
    "one_step = make_train_step(\n",
    "    model_fn=model_fn,\n",
    "    loss_fn=loss_fn,\n",
    "    optax_optimizer=optimizer)\n",
    "\n",
    "\n",
    "# def one_step(weights, opt_state, inputs, labels):\n",
    "#             # compute gradients\n",
    "#     loss, gradients = grad_fn(weights, inputs, labels)\n",
    "#         # compute updates\n",
    "#     updates, opt_state = call_jax(optimizer.update, gradients, opt_state)\n",
    "#         #apply updates\n",
    "#     weights = call_jax(optax.apply_updates, weights, updates)\n",
    "#     return loss, weights, opt_state\n",
    "\n",
    "one_step = torchax.interop.jax_jit(one_step, kwargs_for_jax_jit={'donate_argnums': (0, 2)})\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    global weights\n",
    "    global opt_state\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('jax')\n",
    "        labels = labels.to('jax')\n",
    "\n",
    "        loss, weights, opt_state = one_step(weights, {}, opt_state, inputs, labels) \n",
    "        # Gather data and report\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        if i > 2000: \n",
    "            break\n",
    "            # NOTE: make it run faster for CI\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTrYAn1LsyPE"
   },
   "source": [
    "Per-Epoch Activity\n",
    "==================\n",
    "\n",
    "There are a couple of things we'll want to do once per epoch:\n",
    "\n",
    "-   Perform validation by checking our relative loss on a set of data\n",
    "    that was not used for training, and report this\n",
    "-   Save a copy of the model\n",
    "\n",
    "Here, we'll do our reporting in TensorBoard. This will require going to\n",
    "the command line to start TensorBoard, and opening it in another browser\n",
    "tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfcNC9UwsyPE",
    "outputId": "ea17f7c0-a586-4519-acb5-d49e2923c14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5943: UserWarning: Explicitly requested dtype int64 requested in arange is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return _arange(start, stop=stop, step=step, dtype=dtype,\n",
      "/Users/hanq/git/qihqi/torchax/torchax/ops/mappings.py:83: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:209.)\n",
      "  res = torch.from_numpy(numpy.asarray(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.0007757039815188\n",
      "  batch 2000 loss: 0.6775612101629377\n",
      "LOSS train 0.6775612101629377 valid Tensor(<class 'jaxlib._jax.ArrayImpl'> 0.6449958)\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.6143077593529597\n",
      "  batch 2000 loss: 0.543319463224354\n",
      "LOSS train 0.543319463224354 valid Tensor(<class 'jaxlib._jax.ArrayImpl'> 0.55330473)\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to('jax')\n",
    "            vlabels = vlabels.to('jax')\n",
    "            model.load_state_dict(weights) # put the trained weight back to test it\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            if i > 1000:\n",
    "                break\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model checkpoint\n",
    "\n",
    "Currently `torch.save` (which is based on Pickle) are not able to save tensors on 'jax' device. \n",
    "Because JAX arrays cannot be pickled.\n",
    "\n",
    "So now we have 2 strategies for saving:\n",
    "1. convert the tensors on jax devices to plain JAX arrays; then use flax.checkpoint to save the data. You will get an JAX-style checkpoint (directory) if you do so.\n",
    "2. convert the tensors from jax devices to CPU torch.Tensor, then use `torch.save`; you will get a regular pickle based checkpoint if you do so.\n",
    "\n",
    "We recommend 1. and we have provided wrapper in `torchax.save_checkpoint` that does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import orbax.checkpoint as ocp\n",
    "ckpt_dir = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\n",
    "model_path = ckpt_dir / 'state'\n",
    "torchax.save_checkpoint(weights, model_path, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/my-checkpoints/\n",
      "/tmp/my-checkpoints/state\n",
      "/tmp/my-checkpoints/state/checkpoint_1\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_sharding\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/_CHECKPOINT_METADATA\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas\n",
      "/tmp/my-checkpoints/state/checkpoint_1/array_metadatas/process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/d/a3ddd2e5f397d67e91789ba879d4dd7f\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/manifest.ocdbt\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/d93c12ce6141615627e44d574d6c7277\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/7ea645b957f87cd9df883e2d0f5205bf\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/da7cbbf95d8fa1f2b7ae34daa713bef3\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/d1c362243563d8b2b92c1bcceee0ddaf\n",
      "/tmp/my-checkpoints/state/checkpoint_1/ocdbt.process_0/d/bcb90d6f6ab75dc2b0cfbf48b9301ed1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanq/homebrew/Caskroom/miniconda/base/envs/py13/lib/python3.13/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!find /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also produce a torch pickle based checkpoint by moving the state_dict to CPU\n",
    "\n",
    "You can do so with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpu_state_dict = jax.tree.map(lambda a: a.jax(), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cpu_state_dict, ckpt_dir / 'torch_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mstate\u001b[m\u001b[m                torch_checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/my-checkpoints/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
